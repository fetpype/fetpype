{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Fetpype <p>An Open-Source Pipeline for Reproducible Fetal Brain MRI Analysis</p>"},{"location":"#about","title":"About","text":"<p>Fetpype is a library that aims at facilitating the analysis of fetal brain MRI by integrating the variety of tools commonly used in processing pipelines. Starting from clinical acquisition of fetal brain MRI (T2-weighted fast spin echo sequences), it performs pre-processing, reconstruction, segmentation and surface extraction.</p> <p></p>"},{"location":"#the-tool","title":"The tool","text":"<p>Fetpype aims at integrating a variety of existing tools, available as docker containers into a single, easy to use interface.  It relies on three main components:</p> <ul> <li>Standardized data formatting following the BIDS (Brain Imaging Data Structure) convention.</li> <li>Integration of containerized methods using Docker or Singularity.</li> <li>Chaining of data calls using Nipype, a library for robust integration of heterogeneous neuroimaging pipelines.</li> <li>Simple yaml configuration files generated using Hydra.</li> </ul> <p></p> <p>This package is actively under deployment in a public GitHub repository. If you have any question, trouble with the code or desire included a new feature feel free to open an issue there! If you find a typo, or want to include your new method in fetpype, you can submit a pull request (details on how to include your method in fetpype can be found here).</p>"},{"location":"#quick-start-guide","title":"Quick start guide","text":""},{"location":"#installation","title":"Installation","text":"<p>Clone the latest version of the fetpype repository <pre><code>git clone https://github.com/fetpype/fetpype\n</code></pre></p> <p>Within your desired python environment, install fetpype <pre><code>pip install -e .\n</code></pre></p>"},{"location":"#running-your-first-pipeline","title":"Running your first pipeline","text":""},{"location":"#data-formatting","title":"Data formatting","text":"<p>Start with a BIDS-formatted dataset containing multiple stacks of low-resolution T2-weighted fetal brain MRI. A BIDS formatted folder should look as follows</p> <pre><code>sub-01\n    [ses-01]\n        anat\n            sub-01_[ses-01]_run-1_T2w.nii.gz\n            sub-01_[ses-01]_run-2_T2w.nii.gz\n            ...\n            sub-01_[ses-01]_run-N_T2w.nii.gz\nsub-myname\n    [ses-01]\n        anat\n            sub-myname_[ses-01]_run-1_T2w.nii.gz\n            sub-myname_[ses-01]_run-2_T2w.nii.gz\n            sub-myname_[ses-01]_run-6_T2w.nii.gz\n            sub-myname_[ses-01]_run-7_T2w.nii.gz\n    [ses-02]\n            sub-myname_[ses-01]_run-1_T2w.nii.gz\n            sub-myname_[ses-01]_run-2_T2w.nii.gz\n            sub-myname_[ses-01]_run-3_T2w.nii.gz\n</code></pre> <p>Here, [ses-XX] is an optional tag/folder level. The <code>anat</code> folder will contain the different runs, which are the different stacks acquired for a given subject. You can find a more detailed description on this format in our input data preparation guide. More information about BIDS formatting is available here.</p> <p>The output of the pipeline will be saved in the <code>derivatives</code> folder, which will contain the different steps of the pipeline, also in BIDS format. You can find a more detailed description of the output data structure here.</p>"},{"location":"#choose-what-you-will-run","title":"Choose what you will run","text":"<p>The pipeline that will be run is defined by a structure of config files. A default pipeline, featuring pre-processing (mask extraction, denoising, cropping and masks), reconstruction (using NeSVoR<sup>1</sup>) and segmentation (using BOUNTI<sup>2</sup>) is defined by the following config file.</p> <p>It starts by a master config located at <code>configs/default_docker.yaml</code> with the following structure <pre><code>defaults:\n  - preprocessing/default # Default preprocessing\n  - reconstruction/nesvor # NeSVoR reconstruction -- You can choose between svrtk, nifymic or nesvor\n  - segmentation/bounti   # BOUNTI segmentation     \n  - _self_\ncontainer: \"docker\"       # Running on docker (other option is singularity)\nreconstruction:           # Generic reconstruction arguments\n  output_resolution: 0.8  # Target resolution for reconstruction\nsave_graph: True\n</code></pre> This config defines a pipeline that will run the default preprocessing step (defined in <code>configs/preprocessing/default.yaml</code>), the NeSVoR reconstruction pipeline (defined in <code>configs/reconstruction/nesvor.yaml</code>) followed by the BOUNTI segmentation pipeline (defined in <code>configs/preprocessing/bounti.yaml</code>). Hydra will go in the corresponding folder and load the files to create a global config defined in a nested manner. Changing the pipeline that you want to run is as easy as changing the reconstruction pipeline from <code>nesvor</code> to <code>niftymic</code>. </p> <p>The details of the configs, the attributes and methods implemented is available in this page.</p>"},{"location":"#singularity","title":"Singularity","text":"<p>Fetpype also supports running pipelines using Singularity containers. To run your pipeline with Singularity, ensure that you have Singularity installed and available. Currently, singularity images need to be built manually and saved to a folder. You can indicate the folder in the .yaml file in the \"singularity_path\" field (see configs/default_sg.yaml for an example). The list of images and their name that are needed to run the pipeline is as follows:</p> <ul> <li><code>nesvor.sif</code> for the NeSVoR pipeline (junshenxu/nesvor:v0.5.0)</li> <li><code>niftymic.sif</code> for the NiftyMIC pipeline (from renbem/niftymic:latest)</li> <li><code>svrtk.sif</code> for the SVRTK pipeline (from fetalsvrtk/svrtk:general_auto_amd)</li> <li><code>bounti.sif</code> for the BOUNTI pipeline (from fetalsvrtk/segmentation:general_auto_amd)</li> <li><code>fetpype_utils.sif</code> for the utils pipeline (from gerardmartijuan/fetpype_utils:latest)</li> </ul>"},{"location":"#just-run-it","title":"Just run it!","text":"<p>Once you chose the pipeline that you are going to run, you can then run it by calling  <pre><code>fetpype_run --data &lt;THE_PATH_TO_YOUR_DATA&gt; --out &lt;THE_PATH_TO_YOUR_DATA&gt;/derivatives/fetpype\n</code></pre></p> <p>Additional options are available when calling <code>fetpype_run --help</code>. Then just wait and see your results! </p>"},{"location":"#while-youre-waiting-for-the-results","title":"While you're waiting for the results","text":"<p>Feel free to explore how fetpype works and what it can do!</p> <ul> <li>Output data formatting</li> <li>Running parts of the pipeline</li> <li>How can I include my method in fetpype?</li> </ul>"},{"location":"#reference","title":"Reference","text":"<p>If you used <code>Fetpype</code> in your research, please cite the following paper: <pre><code>@article{sanchez2025fetpype,\n  title={Fetpype: An open-source pipeline for reproducible fetal brain mri analysis},\n  author={Sanchez, Thomas and Mart{\\'\\i}-Juan, Gerard and Meunier, David and Ballester, Miguel Angel Gonzalez and Camara, Oscar and Piella, Gemma and Cuadra, Meritxell Bach and Auzias, Guillaume},\n  journal={arXiv preprint arXiv:2512.17472},\n  year={2025}\n}\n</code></pre></p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Gerard Mart\u00ed-Juan (@GerardMJuan)</li> <li>Thomas Sanchez (@t-sanchez)</li> <li>Guillaume Auzias (@gauzias)</li> <li>David Meunier (@davidmeunier79)</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Fetpype was funded by ERA-NET NEURON in the context of the MULTIFACT project. It received funding under different national agencies:</p> <ul> <li>Swiss National Science Foundation \u2014 grant 31NE30_203977;</li> <li>Spain's Ministry of Science, Innovation and Universities \u2014 grant MCIN/AEI/10.13039/501100011033/</li> <li>French National Research Agency \u2014 grant ANR-21-NEU2-0005;</li> </ul> <p>This project was also supported by the SulcalGRIDS project (ANR-19-CE45-0014) and the SNSF project no. 215641. </p> <p></p> <ol> <li> <p>Junshen Xu and others. NeSVoR: implicit neural representation for slice-to-volume reconstruction in MRI. IEEE Transactions on Medical Imaging, 2023.\u00a0\u21a9</p> </li> <li> <p>Alena U Uus and others. BOUNTI: brain volumetry and automated parcellation for 3d fetal mri. bioRxiv, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api_nodes/","title":"Nodes","text":""},{"location":"api_nodes/#fetpype.nodes.dhcp","title":"<code>dhcp</code>","text":"<p>Nodes that implement the dHCP pipeline for fetal data.</p> <p>Version from https://github.com/GerardMJuan/dhcp-structural-pipeline , which is a fork of the original dataset https://github.com/BioMedIA/dhcp-structural-pipeline with several fixes and changes</p> <p>The docker image, where everything works \"well\", is: https://hub.docker.com/r/gerardmartijuan/dhcp-pipeline-multifact</p> <p>TODO: specify the changes from one version to another.</p>"},{"location":"api_nodes/#fetpype.nodes.dhcp.dhcp_pipeline","title":"<code>dhcp_pipeline(T2, mask, gestational_age, pre_command='', dhcp_image='', threads=1, flag='all')</code>","text":"<p>Run the dhcp segmentation pipeline on a single subject. The script needs to create the output folders and put the mask there so that the docker image can find it and doesn't run bet. TODO: don't do it that convoluted. TODO: Be able to input the number of threads</p>"},{"location":"api_nodes/#fetpype.nodes.dhcp.dhcp_pipeline--flags-can-be-either-all-seg-or-surf","title":"Flags can be either \"-all\", \"-seg\", or \"-surf\"","text":"Source code in <code>fetpype/nodes/dhcp.py</code> <pre><code>def dhcp_pipeline(\n    T2,\n    mask,\n    gestational_age,\n    pre_command=\"\",\n    dhcp_image=\"\",\n    threads=1,\n    flag=\"all\",\n):\n    \"\"\"Run the dhcp segmentation pipeline on a single subject.\n    The script needs to create the output folders and put the mask\n    there so that the docker image can find it and doesn't run bet.\n    TODO: don't do it that convoluted.\n    TODO: Be able to input the number of threads\n\n    # Flags can be either \"-all\", \"-seg\", or \"-surf\"\n    \"\"\"\n    import os\n    import shutil\n\n    output_dir = os.path.abspath(\"dhcp_output\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Basename of the T2 file\n    recon_file_name = os.path.basename(T2)\n\n    # Copy T2 to output dir\n    shutil.copyfile(T2, os.path.join(output_dir, recon_file_name))\n\n    # Copy mask to output dir with the correct name\n    os.makedirs(os.path.join(output_dir, \"segmentations\"), exist_ok=True)\n\n    # check if mask file exists. If not, create it\n    shutil.copyfile(\n        mask,\n        os.path.join(\n            output_dir,\n            \"segmentations\",\n            f\"{recon_file_name.replace('.nii.gz', '')}_brain_mask.nii.gz\",\n        ),\n    )\n\n    if \"docker\" in pre_command:\n        cmd = pre_command\n        cmd += (\n            f\"-v {output_dir}:/data \"\n            f\"{dhcp_image} \"\n            f\"/data/{recon_file_name} \"\n            f\"{gestational_age} \"\n            \"-data-dir /data \"\n            f\"-t {threads} \"\n            \"-c 0 \"\n            f\"{flag} \"\n        )\n\n    elif \"singularity\" in pre_command:\n        # Do we need FSL for this pipeline? add in the precommand\n        cmd = pre_command + dhcp_image\n        cmd += (\n            f\"/usr/local/src/structural-pipeline/fetal-pipeline.sh \"\n            f\"{T2} \"\n            f\"{gestational_age} \"\n            f\"-data-dir \"\n            f\"{output_dir} \"\n            f\"-t {threads} \"\n            \"-c 0 \"\n            f\"{flag} \"\n        )\n\n    else:\n        raise ValueError(\n            \"pre_command must either contain docker or singularity.\"\n        )\n\n    print(cmd)\n    os.system(cmd)\n\n    # assert if the output files exist\n    assert os.path.exists(\n        os.path.join(\n            output_dir,\n            \"segmentations\",\n            f\"{recon_file_name.replace('.nii.gz', '')}_all_labels.nii.gz\",\n        )\n    ), \"Error, segmentations file does not exist\"\n\n    return output_dir\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.preprocessing","title":"<code>preprocessing</code>","text":""},{"location":"api_nodes/#fetpype.nodes.preprocessing.CropStacksAndMasks","title":"<code>CropStacksAndMasks</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Interface to crop the field of view of an image and its mask.</p> <p>This class provides functionality to crop a Nifti image and its corresponding mask to the bounding box defined by the mask. It also allows for adding boundaries around the cropped region.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Input image filename</p> required <code>mask</code> <code>input; str</code> <p>Input mask filename</p> required <code>boundary</code> <code>input; int</code> <p>Padding (in mm) to be set around                     the cropped image and mask.</p> required <code>is_enabled</code> <code>input; bool</code> <p>Whether cropping and masking are enabled.</p> required <code>output_image</code> <code>output; str</code> <p>Path to the cropped image.</p> required <code>output_mask</code> <code>output; str</code> <p>Path to the cropped mask.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from fetpype.nodes.preprocessing import CropStacksAndMasks\n&gt;&gt;&gt; crop_input = CropStacksAndMasks()\n&gt;&gt;&gt; crop_input.inputs.image = 'sub-01_acq-haste_run-1_T2w.nii.gz'\n&gt;&gt;&gt; crop_input.inputs.mask = 'sub-01_acq-haste_run-1_T2w_mask.nii.gz'\n&gt;&gt;&gt; crop_input.run()\n</code></pre> References <ul> <li>Michael Ebner's NiftyMIC repository: https://github.com/gift-surg/NiftyMIC</li> </ul> Source code in <code>fetpype/nodes/preprocessing.py</code> <pre><code>class CropStacksAndMasks(BaseInterface):\n    \"\"\"\n    Interface to crop the field of view of an image and its mask.\n\n    This class provides functionality to crop a Nifti image\n    and its corresponding mask to the bounding box defined by\n    the mask. It also allows for adding boundaries around the\n    cropped region.\n\n    Args:\n        image (str): Input image filename\n        mask (input; str): Input mask filename\n        boundary (input; int):  Padding (in mm) to be set around\n                                the cropped image and mask.\n        is_enabled (input; bool): Whether cropping and masking are enabled.\n        output_image (output; str): Path to the cropped image.\n        output_mask (output; str): Path to the cropped mask.\n\n    Examples:\n        &gt;&gt;&gt; from fetpype.nodes.preprocessing import CropStacksAndMasks\n        &gt;&gt;&gt; crop_input = CropStacksAndMasks()\n        &gt;&gt;&gt; crop_input.inputs.image = 'sub-01_acq-haste_run-1_T2w.nii.gz'\n        &gt;&gt;&gt; crop_input.inputs.mask = 'sub-01_acq-haste_run-1_T2w_mask.nii.gz'\n        &gt;&gt;&gt; crop_input.run() # doctest: +SKIP\n\n    References:\n        - Michael Ebner's NiftyMIC repository:\n        https://github.com/gift-surg/NiftyMIC\n    \"\"\"\n\n    input_spec = CropStacksAndMasksInputSpec\n    output_spec = CropStacksAndMasksOutputSpec\n\n    def _gen_filename(self, name):\n        if name == \"output_image\":\n            return os.path.abspath(os.path.basename(self.inputs.image))\n        elif name == \"output_mask\":\n            return os.path.abspath(os.path.basename(self.inputs.mask))\n        return None\n\n    def _crop_stack_and_mask(\n        self,\n        image_path,\n        mask_path,\n        boundary_i=0,\n        boundary_j=0,\n        boundary_k=0,\n        unit=\"mm\",\n    ):\n        \"\"\"\n        Crops the input image to the field of view given by the bounding box\n        around its mask.\n\n        Args:\n            image_path (str): Path to a Nifti image.\n            mask_path (str): Path to the corresponding Nifti mask.\n            boundary_i (int):   Boundary to add to the bounding box in\n                                the i direction.\n            boundary_j (int):   Boundary to add to the bounding box in\n                                the j direction.\n            boundary_k (int):   Boundary to add to the bounding box in\n                                the k direction.\n            unit (str): The unit defining the dimension size in Nifti.\n\n        Returns:\n            image_cropped:  Image cropped to the bounding box of mask_ni,\n                            including boundary.\n            mask_cropped: Mask cropped to its bounding box.\n\n        Notes:\n            Code inspired by Michael Ebner:\n            https://github.com/gift-surg/NiftyMIC/blob/master/niftymic/base/stack.py\n        \"\"\"\n\n        log.info(f\"Working on {image_path} and {mask_path}\")\n        image_ni = ni.load(image_path)\n        mask_ni = ni.load(mask_path)\n\n        image = image_ni.get_fdata()\n        mask = mask_ni.get_fdata()\n\n        assert all([i &gt;= m] for i, m in zip(image.shape, mask.shape)), (\n            \"For a correct cropping, the image should be larger \"\n            \"or equal to the mask.\"\n        )\n\n        # Get rectangular region surrounding the masked voxels\n        [x_range, y_range, z_range] = self._get_rectangular_masked_region(mask)\n\n        if np.array([x_range, y_range, z_range]).all() is None:\n            log.warning(\n                \"Cropping to bounding box of mask led to an empty image.\"\n            )\n            return None\n\n        if unit == \"mm\":\n            spacing = image_ni.header.get_zooms()\n            boundary_i = np.round(boundary_i / float(spacing[0]))\n            boundary_j = np.round(boundary_j / float(spacing[1]))\n            boundary_k = np.round(boundary_k / float(spacing[2]))\n\n        shape = [min(im, m) for im, m in zip(image.shape, mask.shape)]\n        x_range[0] = np.max([0, x_range[0] - boundary_i])\n        x_range[1] = np.min([shape[0], x_range[1] + boundary_i])\n\n        y_range[0] = np.max([0, y_range[0] - boundary_j])\n        y_range[1] = np.min([shape[1], y_range[1] + boundary_j])\n\n        z_range[0] = np.max([0, z_range[0] - boundary_k])\n        z_range[1] = np.min([shape[2], z_range[1] + boundary_k])\n\n        new_origin = list(\n            ni.affines.apply_affine(\n                image_ni.affine, [x_range[0], y_range[0], z_range[0]]\n            )\n        ) + [1]\n\n        new_affine = image_ni.affine\n        new_affine[:, -1] = new_origin\n\n        image_cropped = image[\n            x_range[0] : x_range[1],  # noqa: E203\n            y_range[0] : y_range[1],  # noqa: E203\n            z_range[0] : z_range[1],  # noqa: E203\n        ]\n        mask_cropped = mask[\n            x_range[0] : x_range[1],  # noqa: E203\n            y_range[0] : y_range[1],  # noqa: E203\n            z_range[0] : z_range[1],  # noqa: E203\n        ]\n\n        image_cropped = ni.Nifti1Image(image_cropped, new_affine)\n        mask_cropped = ni.Nifti1Image(mask_cropped, new_affine)\n        ni.save(image_cropped, self._gen_filename(\"output_image\"))\n        ni.save(mask_cropped, self._gen_filename(\"output_mask\"))\n\n    def _get_rectangular_masked_region(\n        self,\n        mask: np.ndarray,\n    ) -&gt; tuple:\n        \"\"\"\n        Computes the bounding box around the given mask.\n        Code inspired by Michael Ebner:\n        https://github.com/gift-surg/NiftyMIC/blob/master/niftymic/base/stack.py\n\n        Args:\n            mask (np.ndarray): Input mask.\n            range_x (tuple): Pair defining x interval of mask in voxel space.\n            range_y (tuple): Pair defining y interval of mask in voxel space.\n            range_z (tuple): Pair defining z interval of mask in voxel space.\n\n        Returns:\n            tuple: A tuple containing the bounding box ranges for x, y, and z.\n\n        \"\"\"\n        if np.sum(abs(mask)) == 0:\n            return None, None, None\n        shape = mask.shape\n        # Define the dimensions along which to sum the data\n        sum_axis = [(1, 2), (0, 2), (0, 1)]\n        range_list = []\n\n        # Non-zero elements of numpy array along the the 3 dimensions\n        for i in range(3):\n            sum_mask = np.sum(mask, axis=sum_axis[i])\n            ran = np.nonzero(sum_mask)[0]\n\n            low = np.max([0, ran[0]])\n            high = np.min([shape[0], ran[-1] + 1])\n            range_list.append(np.array([low, high]).astype(int))\n\n        return range_list\n\n    def _run_interface(self, runtime):\n        if self.inputs.is_enabled:\n            boundary = self.inputs.boundary\n            self._crop_stack_and_mask(\n                self.inputs.image,\n                self.inputs.mask,\n                boundary_i=boundary,\n                boundary_j=boundary,\n                boundary_k=boundary,\n            )\n        else:\n            os.system(\n                f\"cp {self.inputs.image} \"\n                f\"{self._gen_filename('output_image')}\"\n            )\n            os.system(\n                f\"cp {self.inputs.mask} \"\n                f\"{self._gen_filename('output_mask')}\"\n            )\n\n    def _list_outputs(self):\n        outputs = self._outputs().get()\n        outputs[\"output_image\"] = self._gen_filename(\"output_image\")\n        outputs[\"output_mask\"] = self._gen_filename(\"output_mask\")\n        return outputs\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.preprocessing.CheckAffineResStacksAndMasks","title":"<code>CheckAffineResStacksAndMasks</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Interface to check that the shape of stacks and masks are consistent. (e.g. no trailing dimensions of size 1) and stack ordering in the last dimension. If enabled, also checks that the resolution, affine, and shape of the stacks and masks are consistent. Discards the stack and mask if they are not.</p> <p>Args:</p> <pre><code>stacks (input; list): List of input stacks.\nmasks (input; list): List of input masks.\nis_enabled (input; bool): Whether the check is enabled.\noutput_stacks (output; list): List of stacks that passed the check.\noutput_masks (output; list): List of masks that passed the check.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from fetpype.nodes.preprocessing import CheckAffineResStacksAndMasks # noqa: E501\n&gt;&gt;&gt; check_input = CheckAffineResStacksAndMasks()\n&gt;&gt;&gt; check_input.inputs.stacks = ['sub-01_acq-haste_run-1_T2w.nii.gz']\n&gt;&gt;&gt; check_input.inputs.masks = ['sub-01_acq-haste_run-1_T2w_mask.nii.gz']  # noqa: E501\n&gt;&gt;&gt; check_input.run()\n</code></pre> Source code in <code>fetpype/nodes/preprocessing.py</code> <pre><code>class CheckAffineResStacksAndMasks(BaseInterface):\n    \"\"\"\n    Interface to check that the shape of stacks and masks are consistent.\n    (e.g. no trailing dimensions of size 1) and stack ordering in the last dimension.\n    If enabled, also checks that the resolution, affine, and shape of the\n    stacks and masks are consistent. Discards the stack and mask if they are\n    not.\n\n    Args:\n\n        stacks (input; list): List of input stacks.\n        masks (input; list): List of input masks.\n        is_enabled (input; bool): Whether the check is enabled.\n        output_stacks (output; list): List of stacks that passed the check.\n        output_masks (output; list): List of masks that passed the check.\n\n    Examples:\n        &gt;&gt;&gt; from fetpype.nodes.preprocessing import CheckAffineResStacksAndMasks # noqa: E501\n        &gt;&gt;&gt; check_input = CheckAffineResStacksAndMasks()\n        &gt;&gt;&gt; check_input.inputs.stacks = ['sub-01_acq-haste_run-1_T2w.nii.gz']\n        &gt;&gt;&gt; check_input.inputs.masks = ['sub-01_acq-haste_run-1_T2w_mask.nii.gz']  # noqa: E501\n        &gt;&gt;&gt; check_input.run() # doctest: +SKIP\n    \"\"\"\n\n    input_spec = CheckAffineResStacksAndMasksInputSpec\n    output_spec = CheckAffineResStacksAndMasksOutputSpec\n    _results = {}\n\n    def _squeeze_dim(self, arr, dim):\n        if arr.shape[dim] == 1 and len(arr.shape) &gt; 3:\n            return np.squeeze(arr, axis=dim)\n        return arr\n\n    def compare_resolution_affine(self, r1, a1, r2, a2, s1, s2) -&gt; bool:\n        r1 = np.array(r1)\n        a1 = np.array(a1)\n        r2 = np.array(r2)\n        a2 = np.array(a2)\n        if s1 != s2:\n            return False\n        if r1.shape != r2.shape:\n            return False\n        if np.amax(np.abs(r1 - r2)) &gt; 1e-3:\n            return False\n        if a1.shape != a2.shape:\n            return False\n        if np.amax(np.abs(a1 - a2)) &gt; 1e-3:\n            return False\n        return True\n\n    def check_inplane_pos(self, path, r1):\n        \"\"\"\n        Check if the smallest dimension of the stack is the last one.\n        \"\"\"\n        vx_str = \" x \".join([f\"{v:.2f}\" for v in r1])\n        assert r1[0] == r1[1], (\n            f\"Inconsistent voxel sizes at dimensions 0 and 1 \"\n            f\"for {path} (voxel size = ({vx_str})). \"\n            \"Are you sure that the data are \"\n            f\"formatted as in-plane x in-plane x through-plane?\"\n        )\n\n    def _run_interface(self, runtime):\n        stacks_out = []\n        masks_out = []\n        for i, (imp, maskp) in enumerate(\n            zip(self.inputs.stacks, self.inputs.masks)\n        ):\n            skip_stack = False\n            out_stack = os.path.join(\n                self._gen_filename(\"output_dir\"), os.path.basename(imp)\n            )\n            out_mask = os.path.join(\n                self._gen_filename(\"output_dir\"),\n                os.path.basename(maskp),\n            )\n            image_ni = ni.load(self.inputs.stacks[i])\n            mask_ni = ni.load(self.inputs.masks[i])\n            image = self._squeeze_dim(image_ni.get_fdata(), -1)\n            mask = self._squeeze_dim(mask_ni.get_fdata(), -1)\n            image_ni = ni.Nifti1Image(image, image_ni.affine, image_ni.header)\n            mask_ni = ni.Nifti1Image(mask, mask_ni.affine, mask_ni.header)\n\n            if self.inputs.is_enabled:\n                im_res = image_ni.header[\"pixdim\"][1:4]\n                mask_res = mask_ni.header[\"pixdim\"][1:4]\n                im_aff = image_ni.affine\n                mask_aff = mask_ni.affine\n                im_shape = image_ni.shape\n                mask_shape = mask_ni.shape\n                self.check_inplane_pos(self.inputs.stacks[i], im_res)\n\n                if not self.compare_resolution_affine(\n                    im_res, im_aff, mask_res, mask_aff, im_shape, mask_shape\n                ):\n                    skip_stack = True\n                    print(\n                        f\"Resolution/shape/affine mismatch -- \"\n                        f\"Skipping the stack {os.path.basename(imp)} \"\n                        f\"and mask {os.path.basename(maskp)}\"\n                    )\n                if mask.sum() == 0:\n                    skip_stack = True\n                    print(\n                        f\"Mask {os.path.basename(maskp)} is empty -- \"\n                        f\"Skipping the stack {os.path.basename(imp)} \"\n                        f\"and mask {os.path.basename(maskp)}\"\n                    )\n\n            if not skip_stack:\n                ni.save(image_ni, out_stack)\n                ni.save(mask_ni, out_mask)\n                stacks_out.append(str(out_stack))\n                masks_out.append(str(out_mask))\n        self._results[\"output_stacks\"] = stacks_out\n        self._results[\"output_masks\"] = masks_out\n        if len(stacks_out) == 0:\n            raise ValueError(\n                \"All stacks and masks were \"\n                \"discarded during the metadata check.\"\n            )\n        return runtime\n\n    def _gen_filename(self, name):\n\n        if name == \"output_dir\":\n            return os.path.abspath(\"\")\n        return None\n\n    def _list_outputs(self):\n        outputs = self._outputs().get()\n        outputs[\"output_stacks\"] = self._results.get(\n            \"output_stacks\", self._gen_filename(\"output_stacks\")\n        )\n        outputs[\"output_masks\"] = self._results.get(\n            \"output_masks\", self._gen_filename(\"output_masks\")\n        )\n        return outputs\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.preprocessing.CheckAffineResStacksAndMasks.check_inplane_pos","title":"<code>check_inplane_pos(path, r1)</code>","text":"<p>Check if the smallest dimension of the stack is the last one.</p> Source code in <code>fetpype/nodes/preprocessing.py</code> <pre><code>def check_inplane_pos(self, path, r1):\n    \"\"\"\n    Check if the smallest dimension of the stack is the last one.\n    \"\"\"\n    vx_str = \" x \".join([f\"{v:.2f}\" for v in r1])\n    assert r1[0] == r1[1], (\n        f\"Inconsistent voxel sizes at dimensions 0 and 1 \"\n        f\"for {path} (voxel size = ({vx_str})). \"\n        \"Are you sure that the data are \"\n        f\"formatted as in-plane x in-plane x through-plane?\"\n    )\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.preprocessing.CheckAndSortStacksAndMasks","title":"<code>CheckAndSortStacksAndMasks</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Interface to check the input stacks and masks and make sure that all stacks have a corresponding mask.</p> <p>Args:</p> <pre><code>stacks (input; list): List of input stacks.\nmasks (input; list): List of input masks.\n\noutput_stacks (output; list): List of stacks that passed the check.\noutput_masks (output; list): List of masks that passed the check.\n</code></pre> <p>Examples:     &gt;&gt;&gt; from fetpype.nodes.preprocessing import CheckAndSortStacksAndMasks     &gt;&gt;&gt; check_input = CheckAndSortStacksAndMasks()     &gt;&gt;&gt; check_input.inputs.stacks = ['sub-01_acq-haste_run-1_T2w.nii.gz']     &gt;&gt;&gt; check_input.inputs.masks = ['sub-01_acq-haste_run-1_mask.nii.gz']     &gt;&gt;&gt; check_input.run() # doctest: +SKIP</p> Source code in <code>fetpype/nodes/preprocessing.py</code> <pre><code>class CheckAndSortStacksAndMasks(BaseInterface):\n    \"\"\"\n    Interface to check the input stacks and masks and make sure that\n    all stacks have a corresponding mask.\n\n    Args:\n\n        stacks (input; list): List of input stacks.\n        masks (input; list): List of input masks.\n\n        output_stacks (output; list): List of stacks that passed the check.\n        output_masks (output; list): List of masks that passed the check.\n    Examples:\n        &gt;&gt;&gt; from fetpype.nodes.preprocessing import CheckAndSortStacksAndMasks\n        &gt;&gt;&gt; check_input = CheckAndSortStacksAndMasks()\n        &gt;&gt;&gt; check_input.inputs.stacks = ['sub-01_acq-haste_run-1_T2w.nii.gz']\n        &gt;&gt;&gt; check_input.inputs.masks = ['sub-01_acq-haste_run-1_mask.nii.gz']\n        &gt;&gt;&gt; check_input.run() # doctest: +SKIP\n    \"\"\"\n\n    input_spec = CheckAndSortStacksAndMasksInputSpec\n    output_spec = CheckAndSortStacksAndMasksOutputSpec\n    _results = {}\n\n    def _run_interface(self, runtime):\n\n        # Check that stacks and masks run_ids match\n        stacks_run = get_run_id(self.inputs.stacks)\n        masks_run = get_run_id(self.inputs.masks)\n\n        out_stacks = []\n        out_masks = []\n        for i, s in enumerate(stacks_run):\n            in_stack = self.inputs.stacks[i]\n\n            if s in masks_run:\n                out_stack = os.path.join(\n                    self._gen_filename(\"output_dir_stacks\"),\n                    os.path.basename(in_stack),\n                )\n                in_mask = self.inputs.masks[masks_run.index(s)]\n                out_mask = os.path.join(\n                    self._gen_filename(\"output_dir_masks\"),\n                    os.path.basename(in_mask),\n                )\n                out_stacks.append(out_stack)\n                out_masks.append(out_mask)\n            else:\n                raise RuntimeError(\n                    f\"Stack {os.path.basename(self.inputs.stacks[i])} has \"\n                    f\"no corresponding mask (existing IDs: {masks_run}).\"\n                )\n\n            os.system(f\"cp {in_stack} \" f\"{out_stack}\")\n            os.system(f\"cp {in_mask} \" f\"{out_mask}\")\n        self._results[\"output_stacks\"] = out_stacks\n        self._results[\"output_masks\"] = out_masks\n        return runtime\n\n    def _gen_filename(self, name):\n        if name == \"output_dir_stacks\":\n            path = os.path.abspath(\"stacks\")\n            os.makedirs(path, exist_ok=True)\n            return path\n        elif name == \"output_dir_masks\":\n            path = os.path.abspath(\"masks\")\n            os.makedirs(path, exist_ok=True)\n            return path\n        return None\n\n    def _list_outputs(self):\n        outputs = self._outputs().get()\n        outputs[\"output_stacks\"] = self._results[\"output_stacks\"]\n        outputs[\"output_masks\"] = self._results[\"output_masks\"]\n        return outputs\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.preprocessing.run_prepro_cmd","title":"<code>run_prepro_cmd(input_stacks, cmd, is_enabled=True, input_masks=None, singularity_path=None, singularity_mount=None)</code>","text":"<p>Run a preprocessing command on input stacks and masks.</p> <p>Parameters:</p> Name Type Description Default <code>input_stacks</code> <code>str or list</code> <p>Input stacks to process.</p> required <code>cmd</code> <code>str</code> <p>Command to run, with tags for input and output.</p> required <code>is_enabled</code> <code>bool</code> <p>Whether the command should be executed.</p> <code>True</code> <code>input_masks</code> <code>str or list</code> <p>Input masks to process.</p> <code>None</code> <code>singularity_path</code> <code>str</code> <p>Path to the Singularity executable.</p> <code>None</code> <code>singularity_mount</code> <code>str</code> <p>Mount point for Singularity.</p> <code>None</code> <p>Returns:     tuple: Output stacks and masks, if specified in the command.            If only one of them is specified, returns that one.            If none are specified, returns None.</p> Source code in <code>fetpype/nodes/preprocessing.py</code> <pre><code>def run_prepro_cmd(\n    input_stacks,\n    cmd,\n    is_enabled=True,\n    input_masks=None,\n    singularity_path=None,\n    singularity_mount=None,\n):\n    \"\"\"\n    Run a preprocessing command on input stacks and masks.\n\n    Args:\n        input_stacks (str or list): Input stacks to process.\n        cmd (str): Command to run, with tags for input and output.\n        is_enabled (bool): Whether the command should be executed.\n        input_masks (str or list, optional): Input masks to process.\n        singularity_path (str, optional): Path to the Singularity executable.\n        singularity_mount (str, optional): Mount point for Singularity.\n    Returns:\n        tuple: Output stacks and masks, if specified in the command.\n               If only one of them is specified, returns that one.\n               If none are specified, returns None.\n\n    \"\"\"\n    import os\n    from fetpype import VALID_PREPRO_TAGS\n    from fetpype.utils.logging import run_and_tee\n\n    # Important for mapnodes\n    unlist_stacks = False\n    unlist_masks = False\n\n    if isinstance(input_stacks, str):\n        input_stacks = [input_stacks]\n        unlist_stacks = True\n    if isinstance(input_masks, str):\n        input_masks = [input_masks]\n        unlist_masks = True\n\n    from fetpype.nodes import is_valid_cmd, get_directory, get_mount_docker\n\n    is_valid_cmd(cmd, VALID_PREPRO_TAGS)\n    if \"&lt;output_stacks&gt;\" not in cmd and \"&lt;output_masks&gt;\" not in cmd:\n        raise RuntimeError(\n            \"No output stacks or masks specified in the command. \"\n            \"Please specify &lt;output_stacks&gt; and/or &lt;output_masks&gt;.\"\n        )\n\n    if is_enabled:\n        output_dir = os.path.join(os.getcwd(), \"output\")\n        in_stacks_dir = get_directory(input_stacks)\n        in_stacks = \" \".join(input_stacks)\n\n        in_masks = \"\"\n        in_masks_dir = None\n        if input_masks is not None:\n            in_masks_dir = get_directory(input_masks)\n            in_masks = \" \".join(input_masks)\n\n        output_stacks = None\n        output_masks = None\n\n        # In cmd, there will be things contained in &lt;&gt;.\n        # Check that everything that is in &lt;&gt; is in valid_tags\n        # If not, raise an error\n\n        # Replace the tags in the command\n        cmd = cmd.replace(\"&lt;input_stacks&gt;\", in_stacks)\n        cmd = cmd.replace(\"&lt;input_masks&gt;\", in_masks)\n        if \"&lt;output_stacks&gt;\" in cmd:\n            output_stacks = [\n                os.path.join(output_dir, os.path.basename(stack))\n                for stack in input_stacks\n            ]\n            cmd = cmd.replace(\"&lt;output_stacks&gt;\", \" \".join(output_stacks))\n        if \"&lt;output_masks&gt;\" in cmd:\n            if input_masks:\n                output_masks = [\n                    os.path.join(output_dir, os.path.basename(mask))\n                    for mask in input_masks\n                ]\n            else:\n                output_masks = [\n                    os.path.join(output_dir, os.path.basename(stack)).replace(\n                        \"_T2w\", \"_mask\"\n                    )\n                    for stack in input_stacks\n                ]\n            cmd = cmd.replace(\"&lt;output_masks&gt;\", \" \".join(output_masks))\n\n        if \"&lt;mount&gt;\" in cmd:\n            mount_cmd = get_mount_docker(\n                in_stacks_dir, in_masks_dir, output_dir\n            )\n            cmd = cmd.replace(\"&lt;mount&gt;\", mount_cmd)\n        if \"&lt;singularity_path&gt;\" in cmd:\n            # assume that if we have a singularity path,\n            # we are using singularity and the\n            # parameter has been set in the config file\n            cmd = cmd.replace(\"&lt;singularity_path&gt;\", singularity_path)\n        if \"&lt;singularity_mount&gt;\" in cmd:\n            # assume that if we have a singularity mount path,\n            # we are using singularity and the\n            # parameter has been set in the config file\n            cmd = cmd.replace(\"&lt;singularity_mount&gt;\", singularity_mount)\n\n        run_and_tee(cmd)\n\n    else:\n        output_stacks = input_stacks if \"&lt;output_stacks&gt;\" in cmd else None\n        output_masks = input_masks if \"&lt;output_masks&gt;\" in cmd else None\n\n    if output_stacks is not None and unlist_stacks:\n        assert (\n            len(output_stacks) == 1\n        ), \"More than one stack was returned, but unlist_stacks is True.\"\n        output_stacks = output_stacks[0]\n    if output_masks is not None and unlist_masks:\n        assert (\n            len(output_masks) == 1\n        ), \"More than one mask was returned, but unlist_masks is True.\"\n        output_masks = output_masks[0]\n    if output_stacks is not None and output_masks is not None:\n        return output_stacks, output_masks\n    elif output_stacks is not None:\n        return output_stacks\n    elif output_masks is not None:\n        return output_masks\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.reconstruction","title":"<code>reconstruction</code>","text":""},{"location":"api_nodes/#fetpype.nodes.reconstruction.run_recon_cmd","title":"<code>run_recon_cmd(input_stacks, input_masks, cmd, cfg, singularity_path=None, singularity_mount=None)</code>","text":"<p>Run a reconstruction command with the given input stacks and masks.</p> <p>Args:</p> <pre><code>input_stacks (list): List of input stack file paths.\ninput_masks (list): List of input mask file paths.\ncmd (str): Command to run, with placeholders for input and output.\ncfg (object): Configuration object containing output directory\n                and resolution.\nsingularity_path (str, optional): Path to the Singularity executable.\nsingularity_mount (str, optional): Mount point for Singularity.\n</code></pre> <p>Returns:     str: Path to the output volume after reconstruction.</p> Source code in <code>fetpype/nodes/reconstruction.py</code> <pre><code>def run_recon_cmd(\n    input_stacks,\n    input_masks,\n    cmd,\n    cfg,\n    singularity_path=None,\n    singularity_mount=None,\n):\n    \"\"\"\n    Run a reconstruction command with the given input stacks and masks.\n\n    Args:\n\n        input_stacks (list): List of input stack file paths.\n        input_masks (list): List of input mask file paths.\n        cmd (str): Command to run, with placeholders for input and output.\n        cfg (object): Configuration object containing output directory\n                        and resolution.\n        singularity_path (str, optional): Path to the Singularity executable.\n        singularity_mount (str, optional): Mount point for Singularity.\n    Returns:\n        str: Path to the output volume after reconstruction.\n    \"\"\"\n    import os\n    import numpy as np\n    import nibabel as nib\n    import traceback\n    from fetpype import VALID_RECON_TAGS as VALID_TAGS\n    from fetpype.nodes import is_valid_cmd, get_directory, get_mount_docker\n    from fetpype.utils.logging import run_and_tee\n\n    is_valid_cmd(cmd, VALID_TAGS)\n    output_dir = os.path.join(os.getcwd(), \"recon\")\n    output_volume = os.path.join(output_dir, \"recon.nii.gz\")\n    in_stacks_dir = get_directory(input_stacks)\n    in_stacks = \" \".join(input_stacks)\n    in_masks_dir = get_directory(input_masks)\n    in_masks = \" \".join(input_masks)\n\n    # In cmd, there will be things contained in &lt;&gt;.\n    # Check that everything that is in &lt;&gt; is in valid_tags\n    # If not, raise an error\n\n    # Replace the tags in the command\n    cmd = cmd.replace(\"&lt;input_stacks&gt;\", in_stacks)\n    cmd = cmd.replace(\"&lt;input_dir&gt;\", in_stacks_dir)\n    cmd = cmd.replace(\"&lt;input_masks&gt;\", in_masks)\n    cmd = cmd.replace(\"&lt;input_masks_dir&gt;\", in_masks_dir)\n    if \"&lt;output_volume&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;output_volume&gt;\", output_volume)\n    if \"&lt;output_dir&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;output_dir&gt;\", output_dir)\n        # Assert that args.path_to_output is defined\n        assert cfg.path_to_output is not None, (\n            \"&lt;output_dir&gt; found in the command of reconstruction, \"\n            \"but path_to_output is not defined.\"\n        )\n        output_volume = os.path.join(output_dir, cfg.path_to_output)\n    if \"&lt;input_tp&gt;\" in cmd:\n        try:\n            input_tp = np.round(\n                np.mean(\n                    [\n                        nib.load(stack).header.get_zooms()[2]\n                        for stack in input_stacks\n                    ]\n                ),\n                1,\n            )\n            cmd = cmd.replace(\"&lt;input_tp&gt;\", str(input_tp))\n        except Exception as e:\n\n            raise ValueError(\n                f\"Error when calculating &lt;input_tp&gt;: {e}\"\n                f\"\\n{traceback.format_exc()}\"\n            )\n\n    if \"&lt;singularity_path&gt;\" in cmd:\n        # assume that if we have a singularity path,\n        # we are using singularity and the\n        # parameter has been set in the config file\n        cmd = cmd.replace(\"&lt;singularity_path&gt;\", singularity_path)\n    if \"&lt;singularity_mount&gt;\" in cmd:\n        # assume that if we have a singularity mount path,\n        # we are using singularity and the\n        # parameter has been set in the config file\n        cmd = cmd.replace(\"&lt;singularity_mount&gt;\", singularity_mount)\n\n    if \"&lt;output_res&gt;\" in cmd:\n        output_res = cfg.output_resolution\n        cmd = cmd.replace(\"&lt;output_res&gt;\", str(output_res))\n    if \"&lt;mount&gt;\" in cmd:\n        mount_cmd = get_mount_docker(in_stacks_dir, in_masks_dir, output_dir)\n        cmd = cmd.replace(\"&lt;mount&gt;\", mount_cmd)\n\n    run_and_tee(cmd)\n    return output_volume\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.segmentation","title":"<code>segmentation</code>","text":""},{"location":"api_nodes/#fetpype.nodes.segmentation.run_seg_cmd","title":"<code>run_seg_cmd(input_srr, cmd, cfg, singularity_path=None, singularity_mount=None, singularity_home=None)</code>","text":"<p>Run a segmentation command with the given input SRR.</p> <p>Parameters:</p> Name Type Description Default <code>input_srr</code> <code>str or list</code> <p>Path to the input SRR file or a list                     containing a single SRR file.</p> required <code>cmd</code> <code>str</code> <p>Command to run, with placeholders for input and output.</p> required <code>cfg</code> <code>object</code> <p>Configuration object containing output directory.</p> required <code>singularity_path</code> <code>str</code> <p>Path to the Singularity executable.</p> <code>None</code> <code>singularity_mount</code> <code>str</code> <p>Mount point for Singularity.</p> <code>None</code> <p>Returns:     str: Path to the output segmentation file after running the command.</p> Source code in <code>fetpype/nodes/segmentation.py</code> <pre><code>def run_seg_cmd(\n    input_srr,\n    cmd,\n    cfg,\n    singularity_path=None,\n    singularity_mount=None,\n    singularity_home=None,\n):\n    \"\"\"\n    Run a segmentation command with the given input SRR.\n\n    Args:\n        input_srr (str or list): Path to the input SRR file or a list\n                                containing a single SRR file.\n        cmd (str): Command to run, with placeholders for input and output.\n        cfg (object): Configuration object containing output directory.\n        singularity_path (str, optional): Path to the Singularity executable.\n        singularity_mount (str, optional): Mount point for Singularity.\n    Returns:\n        str: Path to the output segmentation file after running the command.\n\n    \"\"\"\n    import os\n    from fetpype import VALID_SEG_TAGS as VALID_TAGS\n    from fetpype.nodes import is_valid_cmd, get_mount_docker\n    from fetpype.utils.logging import run_and_tee\n\n    is_valid_cmd(cmd, VALID_TAGS)\n\n    # Check if input_srr is a directory or a file\n    if isinstance(input_srr, list):\n        if len(input_srr) == 1:\n            input_srr = input_srr[0]\n        else:\n            raise ValueError(\n                \"input_srr is a list, and contains multiple elements. \"\n                \"It should be a single element.\"\n            )\n    # Copy input_srr to input_directory\n    # Avoid mounting problematic directories\n    input_srr_dir = os.path.join(os.getcwd(), \"seg/input\")\n    os.makedirs(input_srr_dir, exist_ok=True)\n    os.system(f\"cp {input_srr} {input_srr_dir}/input_srr.nii.gz\")\n    input_srr = os.path.join(input_srr_dir, \"input_srr.nii.gz\")\n\n    output_dir = os.path.join(os.getcwd(), \"seg/out\")\n    seg = os.path.join(output_dir, \"seg.nii.gz\")\n\n    # In cmd, there will be things contained in &lt;&gt;.\n    # Check that everything that is in &lt;&gt; is in valid_tags\n    # If not, raise an error\n\n    # Replace the tags in the command\n    cmd = cmd.replace(\"&lt;input_srr&gt;\", input_srr)\n    cmd = cmd.replace(\"&lt;input_dir&gt;\", input_srr_dir)\n    cmd = cmd.replace(\"&lt;output_seg&gt;\", seg)\n    if \"&lt;output_dir&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;output_dir&gt;\", output_dir)\n        # Assert that args.path_to_output is defined\n        assert cfg.path_to_output is not None, (\n            \"&lt;output_dir&gt; found in the command of reconstruction, \"\n            \" but path_to_output is not defined.\"\n        )\n\n        seg = os.path.join(output_dir, cfg.path_to_output)\n        if \"&lt;basename&gt;\" in seg:\n            # Remove all extensions from the basename\n            # (handles .nii.gz correctly)\n            basename = os.path.basename(input_srr)\n            # Remove all extensions (handles both .nii.gz and .nii cases)\n            basename_no_ext = basename.split(\".\")[0]\n            seg = seg.replace(\"&lt;basename&gt;\", basename_no_ext)\n    if \"&lt;mount&gt;\" in cmd:\n        mount_cmd = get_mount_docker(input_srr_dir, output_dir)\n        cmd = cmd.replace(\"&lt;mount&gt;\", mount_cmd)\n    if \"&lt;singularity_path&gt;\" in cmd:\n        # assume that if we have a singularity path,\n        # we are using singularity and the\n        # parameter has been set in the config file\n        cmd = cmd.replace(\"&lt;singularity_path&gt;\", singularity_path)\n    if \"&lt;singularity_mount&gt;\" in cmd:\n        # assume that if we have a singularity mount path,\n        # we are using singularity and the\n        # parameter has been set in the config file\n        cmd = cmd.replace(\"&lt;singularity_mount&gt;\", singularity_mount)\n\n    if \"&lt;singularity_home&gt;\" in cmd:\n        # assume that if we have a singularity mount path,\n        # we are using singularity and the\n        # parameter has been set in the config file\n        cmd = cmd.replace(\"&lt;singularity_home&gt;\", singularity_home)\n\n    run_and_tee(cmd)\n\n    return seg\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.surface_extraction","title":"<code>surface_extraction</code>","text":""},{"location":"api_nodes/#fetpype.nodes.surface_extraction.run_surf_cmd","title":"<code>run_surf_cmd(input_seg, cmd, cfg, singularity_path=None, singularity_mount=None, singularity_home=None)</code>","text":"<p>Run a segmentation command with the given input SRR.</p> <p>Parameters:</p> Name Type Description Default <code>input_seg</code> <code>str or list</code> <p>Path to the input segmentation file or a list                     containing a single segmentation file.</p> required <code>cmd</code> <code>str</code> <p>Command to run, with placeholders for input and output.</p> required <code>cfg</code> <code>object</code> <p>Configuration object containing output directory.</p> required <code>singularity_path</code> <code>str</code> <p>Path to the Singularity executable.</p> <code>None</code> <code>singularity_mount</code> <code>str</code> <p>Mount point for Singularity.</p> <code>None</code> <p>Returns:     str: Path to the output segmentation file after running the command.</p> Source code in <code>fetpype/nodes/surface_extraction.py</code> <pre><code>def run_surf_cmd(\n    input_seg,\n    cmd,\n    cfg,\n    singularity_path=None,\n    singularity_mount=None,\n    singularity_home=None,\n):\n    \"\"\"\n    Run a segmentation command with the given input SRR.\n\n    Args:\n        input_seg (str or list): Path to the input segmentation file or a list\n                                containing a single segmentation file.\n        cmd (str): Command to run, with placeholders for input and output.\n        cfg (object): Configuration object containing output directory.\n        singularity_path (str, optional): Path to the Singularity executable.\n        singularity_mount (str, optional): Mount point for Singularity.\n    Returns:\n        str: Path to the output segmentation file after running the command.\n\n    \"\"\"\n    import os\n    from fetpype import VALID_SURF_TAGS as VALID_TAGS\n    from fetpype.nodes import is_valid_cmd, get_mount_docker\n    from fetpype.utils.logging import run_and_tee\n\n    is_valid_cmd(cmd, VALID_TAGS)\n\n    # Check if input_srr is a directory or a file\n    if isinstance(input_seg, list):\n        if len(input_seg) == 1:\n            input_seg = input_seg[0]\n        else:\n            raise ValueError(\n                \"input_seg is a list, and contains multiple elements. \"\n                \"It should be a single element.\"\n            )\n    # Copy input_seg to input_directory\n    # Avoid mounting problematic directories\n    input_seg_dir = os.path.join(os.getcwd(), \"seg/input\")\n    os.makedirs(input_seg_dir, exist_ok=True)\n    os.system(f\"cp {input_seg} {input_seg_dir}/input_seg.nii.gz\")\n    input_seg = os.path.join(input_seg_dir, \"input_seg.nii.gz\")\n\n    output_dir = os.path.join(os.getcwd(), \"surf/out\")\n    os.makedirs(output_dir, exist_ok=True)\n    surf = os.path.join(output_dir, cfg.out_file)\n\n    # In cmd, there will be things contained in &lt;&gt;.\n    # Check that everything that is in &lt;&gt; is in valid_tags\n    # If not, raise an error\n\n    # Replace the tags in the command\n    cmd = cmd.replace(\"&lt;input_seg&gt;\", input_seg)\n    # cmd = cmd.replace(\"&lt;input_dir&gt;\", input_seg_dir)\n    cmd = cmd.replace(\"&lt;output_surf&gt;\", surf)\n    assert cfg.use_scheme in cfg.labelling_scheme, (\n        f\"Unknown labelling scheme: {cfg.use_scheme},\"\n        f\"please choose from {list(cfg.labelling_scheme.keys())}\"\n    )\n    labelling_scheme = cfg.labelling_scheme[cfg.use_scheme]\n    labelling_scheme = \",\".join(map(str, labelling_scheme))\n    cmd = cmd.replace(\"&lt;labelling_scheme&gt;\", labelling_scheme)\n\n    # if \"&lt;output_dir&gt;\" in cmd:\n    #     cmd = cmd.replace(\"&lt;output_dir&gt;\", output_dir)\n    #     # Assert that args.path_to_output is defined\n    #     assert cfg.path_to_output is not None, (\n    #         \"&lt;output_dir&gt; found in the command of reconstruction, \"\n    #         \" but path_to_output is not defined.\"\n    #     )\n\n    # seg = os.path.join(output_dir, cfg.path_to_output)\n    # if \"&lt;basename&gt;\" in seg:\n    #     # Remove all extensions from the basename\n    #     # (handles .nii.gz correctly)\n    #     basename = os.path.basename(input_srr)\n    #     # Remove all extensions (handles both .nii.gz and .nii cases)\n    #     basename_no_ext = basename.split(\".\")[0]\n    #     seg = seg.replace(\"&lt;basename&gt;\", basename_no_ext)\n    if \"&lt;mount&gt;\" in cmd:\n        mount_cmd = get_mount_docker(input_seg_dir, output_dir)\n        cmd = cmd.replace(\"&lt;mount&gt;\", mount_cmd)\n    if \"&lt;singularity_path&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;singularity_path&gt;\", singularity_path)\n    if \"&lt;singularity_mount&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;singularity_mount&gt;\", singularity_mount)\n\n    if \"&lt;singularity_home&gt;\" in cmd:\n        cmd = cmd.replace(\"&lt;singularity_home&gt;\", singularity_home)\n\n    run_and_tee(cmd)\n\n    return surf\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.utils","title":"<code>utils</code>","text":""},{"location":"api_nodes/#fetpype.nodes.utils.get_directory","title":"<code>get_directory(entry)</code>","text":"<p>Get the directory of an entry, to be mounted on docker If entry is a list, it returns the common path. If entry is a string, it returns the dirname.</p> Source code in <code>fetpype/nodes/utils.py</code> <pre><code>def get_directory(entry):\n    \"\"\"\n    Get the directory of an entry, to be mounted on docker\n    If entry is a list, it returns the common path.\n    If entry is a string, it returns the dirname.\n    \"\"\"\n    if isinstance(entry, list):\n        if len(entry) == 1:\n            return os.path.dirname(entry[0])\n        return os.path.commonpath(entry)\n\n    elif isinstance(entry, str):\n        return os.path.dirname(entry)\n    else:\n        raise TypeError(f\"Type {type(entry)} not supported\")\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.utils.get_mount_docker","title":"<code>get_mount_docker(*args)</code>","text":"<p>Build the string for the folders to be mounted on the docker image. The folders to be mounted are defined in _mount_keys.</p> Source code in <code>fetpype/nodes/utils.py</code> <pre><code>def get_mount_docker(*args):\n    \"\"\"\n    Build the string for the folders to be mounted on the\n    docker image. The folders to be mounted are defined\n    in _mount_keys.\n    \"\"\"\n    mount_args = []\n    for arg in args:\n        if arg is not None:\n            os.makedirs(arg, exist_ok=True)\n            mount_args.append(arg)\n    return \" \".join([f\"-v {arg}:{arg}\" for arg in mount_args])\n</code></pre>"},{"location":"api_nodes/#fetpype.nodes.utils.get_run_id","title":"<code>get_run_id(file_list)</code>","text":"<p>Get the run ID from the file name.</p> Source code in <code>fetpype/nodes/utils.py</code> <pre><code>def get_run_id(file_list):\n    \"\"\"\n    Get the run ID from the file name.\n    \"\"\"\n    runs = []\n    for file in file_list:\n        try:\n            runs.append(re.search(r\"run-([^\\W_]+)_\", file).group(1))\n        except Exception as e:\n            raise ValueError(\n                f\"run ID not found in file name: {file}. Error: {e}\"\n            )\n    return runs\n</code></pre>"},{"location":"api_pipelines/","title":"Pipelines","text":""},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline","title":"<code>full_pipeline</code>","text":""},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.get_prepro","title":"<code>get_prepro(cfg, load_masks=False, enabled_cropping=False)</code>","text":"<p>Create the preprocessing workflow based on config <code>cfg</code>. Given an input of T2w stacks, this pipeline performs the following steps:     1. Brain extraction using MONAIfbs     2. Check stacks and masks     3. Check affine and resolution of stacks and masks     4. Cropping stacks and masks     5. Denoising stacks     6. Bias field correction of stacks</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <code>load_masks</code> <p>Boolean indicating whether to load masks or         perform brain extraction.</p> <code>False</code> <code>enabled_cropping</code> <p>Boolean indicating whether cropping is enabled.                 This is typically set to False for SVRTK                 pipelines, as they handle cropping internally.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>prepro_pipe</code> <p>A Nipype workflow object that contains             the preprocessing steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def get_prepro(cfg, load_masks=False, enabled_cropping=False):\n    \"\"\"\n    Create the preprocessing workflow based on config `cfg`.\n    Given an input of T2w stacks, this pipeline performs the following steps:\n        1. Brain extraction using MONAIfbs\n        2. Check stacks and masks\n        3. Check affine and resolution of stacks and masks\n        4. Cropping stacks and masks\n        5. Denoising stacks\n        6. Bias field correction of stacks\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n        load_masks: Boolean indicating whether to load masks or\n                    perform brain extraction.\n        enabled_cropping:   Boolean indicating whether cropping is enabled.\n                            This is typically set to False for SVRTK\n                            pipelines, as they handle cropping internally.\n\n    Returns:\n        prepro_pipe:    A Nipype workflow object that contains\n                        the preprocessing steps.\n\n    \"\"\"\n    cfg_prepro = cfg.preprocessing\n\n    prepro_pipe = pe.Workflow(name=\"Preprocessing\")\n    # Creating input node\n\n    enabled_check = cfg_prepro.check_stacks_and_masks.enabled\n    enabled_cropping = cfg_prepro.cropping.enabled and enabled_cropping\n    if cfg_prepro.cropping.enabled != enabled_cropping:\n        print(\"Overriding cropping enabled status for the selected pipeline.\")\n    enabled_denoising = True\n    enabled_bias_corr = cfg_prepro.bias_correction.enabled\n\n    # PREPROCESSING\n    # 0. Define input and outputs\n    in_fields = [\"stacks\"]\n    if load_masks:\n        in_fields += [\"masks\"]\n\n    input = pe.Node(niu.IdentityInterface(fields=in_fields), name=\"inputnode\")\n\n    output = pe.Node(\n        niu.IdentityInterface(fields=[\"stacks\", \"masks\"]), name=\"outputnode\"\n    )\n    # 1. Load masks or brain extraction\n    container = cfg.container\n    if load_masks:\n        check_input = pe.Node(\n            interface=CheckAndSortStacksAndMasks(), name=\"CheckInput\"\n        )\n\n    else:\n        be_config = cfg_prepro.brain_extraction\n        be_cfg_cont = be_config[container]\n\n        brain_extraction = pe.Node(\n            interface=niu.Function(\n                input_names=[\n                    \"input_stacks\",\n                    \"name\",\n                    \"cmd\",\n                    \"singularity_path\",\n                    \"singularity_mount\",\n                ],\n                output_names=[\"output_masks\"],\n                function=run_prepro_cmd,\n            ),\n            name=\"BrainExtraction\",\n        )\n        brain_extraction.inputs.cmd = be_cfg_cont.cmd\n        brain_extraction.inputs.cfg = be_config\n        # if the container is singularity, add\n        # singularity path to the brain_extraction\n        if cfg.container == \"singularity\":\n            brain_extraction.inputs.singularity_path = cfg.singularity_path\n            brain_extraction.inputs.singularity_mount = cfg.singularity_mount\n\n    # 2. Check stacks and masks\n    check_name = \"CheckAffineAndRes\"\n    check_name += \"_disabled\" if not enabled_check else \"\"\n\n    check_affine = pe.Node(\n        interface=CheckAffineResStacksAndMasks(), name=check_name\n    )\n    check_affine.inputs.is_enabled = enabled_check\n    # 3. Cropping\n    cropping_name = \"Cropping\"\n    cropping_name += \"_disabled\" if not enabled_cropping else \"\"\n    cropping = pe.MapNode(\n        interface=CropStacksAndMasks(),\n        iterfield=[\"image\", \"mask\"],\n        name=cropping_name,\n    )\n\n    cropping.inputs.is_enabled = enabled_cropping\n    # 4. Denoising\n    denoising_name = \"Denoising\"\n    denoising_name += \"_disabled\" if not enabled_denoising else \"\"\n\n    denoising = pe.MapNode(\n        interface=niu.Function(\n            input_names=[\n                \"input_stacks\",\n                \"is_enabled\",\n                \"cmd\",\n                \"singularity_path\",\n                \"singularity_mount\",\n            ],\n            output_names=[\"output_stacks\"],\n            function=run_prepro_cmd,\n        ),\n        iterfield=[\"input_stacks\"],\n        name=denoising_name,\n    )\n\n    denoising_cfg = cfg_prepro.denoising\n    denoising.inputs.is_enabled = enabled_denoising\n    denoising.inputs.cmd = denoising_cfg[container].cmd\n    # if the container is singularity, add singularity path to the denoising\n    if cfg.container == \"singularity\":\n        denoising.inputs.singularity_path = cfg.singularity_path\n        denoising.inputs.singularity_mount = cfg.singularity_mount\n\n    merge_denoise = pe.Node(\n        interface=niu.Merge(1, ravel_inputs=True), name=\"MergeDenoise\"\n    )\n    # 5. Bias field correction\n    bias_name = \"BiasCorrection\"\n    bias_name += \"_disabled\" if not enabled_bias_corr else \"\"\n\n    bias_corr = pe.MapNode(\n        interface=niu.Function(\n            input_names=[\n                \"input_stacks\",\n                \"input_masks\",\n                \"is_enabled\",\n                \"cmd\",\n                \"singularity_path\",\n                \"singularity_mount\",\n            ],\n            output_names=[\"output_stacks\"],\n            function=run_prepro_cmd,\n        ),\n        iterfield=[\"input_stacks\", \"input_masks\"],\n        name=bias_name,\n    )\n    bias_cfg = cfg_prepro.bias_correction\n    bias_corr.inputs.is_enabled = enabled_bias_corr\n    bias_corr.inputs.cmd = bias_cfg[container].cmd\n\n    # if the container is singularity, add singularity path to the bias_corr\n    if cfg.container == \"singularity\":\n        bias_corr.inputs.singularity_path = cfg.singularity_path\n        bias_corr.inputs.singularity_mount = cfg.singularity_mount\n\n    # 6. Verify output\n    check_output = pe.Node(\n        interface=CheckAndSortStacksAndMasks(),\n        name=\"CheckOutput\",\n    )\n\n    # Connect nodes\n\n    if load_masks:\n        prepro_pipe.connect(input, \"stacks\", check_input, \"stacks\")\n        prepro_pipe.connect(input, \"masks\", check_input, \"masks\")\n\n        prepro_pipe.connect(\n            check_input, \"output_stacks\", check_affine, \"stacks\"\n        )\n        prepro_pipe.connect(check_input, \"output_masks\", check_affine, \"masks\")\n\n    else:\n        prepro_pipe.connect(input, \"stacks\", brain_extraction, \"input_stacks\")\n\n        prepro_pipe.connect(input, \"stacks\", check_affine, \"stacks\")\n        prepro_pipe.connect(\n            brain_extraction, \"output_masks\", check_affine, \"masks\"\n        )\n\n    prepro_pipe.connect(check_affine, \"output_stacks\", cropping, \"image\")\n    prepro_pipe.connect(check_affine, \"output_masks\", cropping, \"mask\")\n\n    prepro_pipe.connect(cropping, \"output_image\", denoising, \"input_stacks\")\n    prepro_pipe.connect(denoising, \"output_stacks\", merge_denoise, \"in1\")\n\n    prepro_pipe.connect(merge_denoise, \"out\", bias_corr, \"input_stacks\")\n    prepro_pipe.connect(cropping, \"output_mask\", bias_corr, \"input_masks\")\n\n    prepro_pipe.connect(bias_corr, \"output_stacks\", check_output, \"stacks\")\n    prepro_pipe.connect(cropping, \"output_mask\", check_output, \"masks\")\n\n    prepro_pipe.connect(check_output, \"output_stacks\", output, \"stacks\")\n    prepro_pipe.connect(check_output, \"output_masks\", output, \"masks\")\n\n    return prepro_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.get_recon","title":"<code>get_recon(cfg)</code>","text":"<p>Get the reconstruction workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <p>Returns:</p> Name Type Description <code>rec_pipe</code> <p>A Nipype workflow object that contains         the reconstruction steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def get_recon(cfg):\n    \"\"\"\n    Get the reconstruction workflow based on the pipeline specified\n    in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n\n    Returns:\n        rec_pipe:   A Nipype workflow object that contains\n                    the reconstruction steps.\n    \"\"\"\n    rec_pipe = pe.Workflow(name=\"Reconstruction\")\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"stacks\", \"masks\"]), name=\"inputnode\"\n    )\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"srr_volume\"]), name=\"outputnode\"\n    )\n\n    container = cfg.container\n    cfg_reco_base = cfg.reconstruction\n    cfg_reco = cfg.reconstruction[container]\n\n    recon = pe.Node(\n        interface=niu.Function(\n            input_names=[\n                \"input_stacks\",\n                \"input_masks\",\n                \"cmd\",\n                \"cfg\",\n                \"singularity_path\",\n                \"singularity_mount\",\n            ],\n            output_names=[\"srr_volume\"],\n            function=run_recon_cmd,\n        ),\n        name=cfg_reco_base.pipeline,\n    )\n\n    recon.inputs.cmd = cfg_reco.cmd\n    recon.inputs.cfg = cfg_reco_base\n    # if the container is singularity, add singularity path to the recon node\n    if cfg.container == \"singularity\":\n        recon.inputs.singularity_path = cfg.singularity_path\n        recon.inputs.singularity_mount = cfg.singularity_mount\n\n    rec_pipe.connect(\n        [\n            (inputnode, recon, [(\"stacks\", \"input_stacks\")]),\n            (inputnode, recon, [(\"masks\", \"input_masks\")]),\n        ]\n    )\n    rec_pipe.connect(recon, \"srr_volume\", outputnode, \"srr_volume\")\n    return rec_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.get_seg","title":"<code>get_seg(cfg)</code>","text":"<p>Get the segmentation workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <p>Returns:     seg_pipe:   A Nipype workflow object that contains                 the segmentation steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def get_seg(cfg):\n    \"\"\"\n    Get the segmentation workflow based on the pipeline specified\n    in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n    Returns:\n        seg_pipe:   A Nipype workflow object that contains\n                    the segmentation steps.\n    \"\"\"\n    seg_pipe = pe.Workflow(name=\"Segmentation\")\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"srr_volume\"]), name=\"inputnode\"\n    )\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"seg_volume\"]), name=\"outputnode\"\n    )\n\n    container = cfg.container\n    cfg_seg_base = cfg.segmentation\n    cfg_seg = cfg.segmentation[container]\n\n    seg = pe.Node(\n        interface=niu.Function(\n            input_names=[\n                \"input_srr\",\n                \"cmd\",\n                \"cfg\",\n                \"singularity_path\",\n                \"singularity_mount\",\n                \"singularity_home\",\n            ],\n            output_names=[\"seg_volume\"],\n            function=run_seg_cmd,\n        ),\n        name=cfg_seg_base.pipeline,\n    )\n\n    seg.inputs.cmd = cfg_seg.cmd\n    seg.inputs.cfg = cfg_seg_base\n    if cfg.container == \"singularity\":\n        seg.inputs.singularity_path = cfg.singularity_path\n        seg.inputs.singularity_mount = cfg.singularity_mount\n        seg.inputs.singularity_home = cfg.singularity_home\n\n    seg_pipe.connect(inputnode, \"srr_volume\", seg, \"input_srr\")\n    seg_pipe.connect(seg, \"seg_volume\", outputnode, \"seg_volume\")\n\n    return seg_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.get_surf","title":"<code>get_surf(cfg)</code>","text":"<p>Get the surface extraction workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <p>Returns:     surf_pipe:   A Nipype workflow object that contains                 the surface extraction steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def get_surf(cfg):\n    \"\"\"\n    Get the surface extraction workflow based on the pipeline specified\n    in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n    Returns:\n        surf_pipe:   A Nipype workflow object that contains\n                    the surface extraction steps.\n    \"\"\"\n    surf_pipe = pe.Workflow(name=\"SurfaceExtraction\")\n\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"seg_volume\"]), name=\"inputnode\"\n    )\n\n    outputnode = pe.Node(\n        niu.IdentityInterface(\n            fields=[\"surf_volume_lh\", \"surf_volume_rh\"]),\n        name=\"outputnode\"\n    )\n\n    print(cfg)\n\n    container = cfg.container\n    cfg_surf_base = cfg.surface\n    cfg_surf = cfg.surface[container]\n\n    # surf_lh\n    surf_lh = pe.Node(\n        interface=niu.Function(\n            input_names=[\n                \"input_seg\",\n                \"cmd\",\n                \"cfg\",\n                \"singularity_path\",\n                \"singularity_mount\",\n                \"singularity_home\",\n            ],\n            output_names=[\"surf_volume\"],\n            function=run_surf_cmd,\n        ),\n        name=\"surf_lh\",\n    )\n\n    surf_lh.inputs.cmd = cfg_surf.cmd\n    surf_lh.inputs.cfg = cfg_surf_base.surface_lh\n\n    if cfg.container == \"singularity\":\n        surf_lh.inputs.singularity_path = cfg.singularity_path\n        surf_lh.inputs.singularity_mount = cfg.singularity_mount\n        surf_lh.inputs.singularity_home = cfg.singularity_home\n\n    surf_pipe.connect(inputnode, \"seg_volume\", surf_lh, \"input_seg\")\n    surf_pipe.connect(surf_lh, \"surf_volume\", outputnode, \"surf_volume_lh\")\n\n    # surf_rh\n    surf_rh = pe.Node(\n        interface=niu.Function(\n            input_names=[\n                \"input_seg\",\n                \"cmd\",\n                \"cfg\",\n                \"singularity_path\",\n                \"singularity_mount\",\n                \"singularity_home\",\n            ],\n            output_names=[\"surf_volume\"],\n            function=run_surf_cmd,\n        ),\n        name=\"surf_rh\",\n    )\n\n    surf_rh.inputs.cmd = cfg_surf.cmd\n    surf_rh.inputs.cfg = cfg_surf_base.surface_rh\n\n    if cfg.container == \"singularity\":\n        surf_rh.inputs.singularity_path = cfg.singularity_path\n        surf_rh.inputs.singularity_mount = cfg.singularity_mount\n        surf_rh.inputs.singularity_home = cfg.singularity_home\n\n    surf_pipe.connect(inputnode, \"seg_volume\", surf_rh, \"input_seg\")\n    surf_pipe.connect(surf_rh, \"surf_volume\", outputnode, \"surf_volume_rh\")\n\n    return surf_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.create_full_pipeline","title":"<code>create_full_pipeline(cfg, load_masks=False, name='full_pipeline')</code>","text":"<p>Create a full fetal processing pipeline by combining preprocessing, reconstruction, and segmentation workflows.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <code>load_masks</code> <p>Boolean indicating whether to load masks or perform         brain extraction.</p> <code>False</code> <code>name</code> <p>Name of the pipeline (default = \"full_pipeline\").</p> <code>'full_pipeline'</code> <p>Returns:</p> Name Type Description <code>full_fet_pipe</code> <p>A Nipype workflow object that contains             the full pipeline steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def create_full_pipeline(cfg, load_masks=False, name=\"full_pipeline\"):\n    \"\"\"\n    Create a full fetal processing pipeline by combining preprocessing,\n    reconstruction, and segmentation workflows.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n        load_masks: Boolean indicating whether to load masks or perform\n                    brain extraction.\n        name: Name of the pipeline (default = \"full_pipeline\").\n\n    Returns:\n        full_fet_pipe:  A Nipype workflow object that contains\n                        the full pipeline steps.\n\n    \"\"\"\n    print(\"Full pipeline name: \", name)\n    # Creating pipeline\n    full_fet_pipe = pe.Workflow(name=name)\n\n    config.update_config(full_fet_pipe.config)\n    # Creating input node\n\n    in_fields = [\"stacks\"]\n    if load_masks:\n        in_fields += [\"masks\"]\n\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=in_fields), name=\"inputnode\"\n    )\n\n    outputnode = pe.Node(\n        niu.IdentityInterface(\n            fields=[\"output_srr\", \"output_seg\",\n                    \"output_surf_lh\", \"output_surf_rh\"]\n        ),\n        name=\"outputnode\",\n    )\n\n    enabled_cropping = (\n        False if cfg.reconstruction.pipeline == \"svrtk\" else True\n    )\n    prepro_pipe = get_prepro(cfg, load_masks, enabled_cropping)\n    recon = get_recon(cfg)\n    segmentation = get_seg(cfg)\n\n    surface = get_surf(cfg)\n\n    # PREPROCESSING\n    full_fet_pipe.connect(inputnode, \"stacks\", prepro_pipe, \"inputnode.stacks\")\n\n    # RECONSTRUCTION\n    full_fet_pipe.connect(\n        prepro_pipe, \"outputnode.stacks\", recon, \"inputnode.stacks\"\n    )\n\n    full_fet_pipe.connect(\n        prepro_pipe, \"outputnode.masks\", recon, \"inputnode.masks\"\n    )\n\n    full_fet_pipe.connect(\n        recon, \"outputnode.srr_volume\", outputnode, \"output_srr\"\n    )\n\n    # SEGMENTATION\n    full_fet_pipe.connect(\n        recon, \"outputnode.srr_volume\", segmentation, \"inputnode.srr_volume\"\n    )\n\n    full_fet_pipe.connect(\n        segmentation, \"outputnode.seg_volume\", outputnode, \"output_seg\"\n    )\n\n    # SURFACE EXTRACTION\n    full_fet_pipe.connect(\n        segmentation, \"outputnode.seg_volume\", surface, \"inputnode.seg_volume\"\n    )\n\n    full_fet_pipe.connect(\n        surface, \"outputnode.surf_volume_lh\", outputnode, \"output_surf_lh\"\n    )\n\n    full_fet_pipe.connect(\n        surface, \"outputnode.surf_volume_rh\", outputnode, \"output_surf_rh\"\n    )\n\n    return full_fet_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.create_rec_pipeline","title":"<code>create_rec_pipeline(cfg, load_masks=False, name='rec_pipeline')</code>","text":"<p>Create the reconstruction workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <code>load_masks</code> <p>Boolean indicating whether to load masks or perform         brain extraction.</p> <code>False</code> <code>name</code> <p>Name of the pipeline (default = \"rec_pipeline\").</p> <code>'rec_pipeline'</code> <p>Returns:</p> Name Type Description <code>rec_pipe</code> <p>A Nipype workflow object that contains the       reconstruction steps.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def create_rec_pipeline(cfg, load_masks=False, name=\"rec_pipeline\"):\n    \"\"\"\n    Create the reconstruction workflow based on the pipeline specified\n    in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n        load_masks: Boolean indicating whether to load masks or perform\n                    brain extraction.\n        name: Name of the pipeline (default = \"rec_pipeline\").\n\n    Returns:\n        rec_pipe: A Nipype workflow object that contains the\n                  reconstruction steps.\n    \"\"\"\n    print(\"Full pipeline name: \", name)\n    # Creating pipeline\n    rec_pipe = pe.Workflow(name=name)\n    config.update_config(rec_pipe.config)\n    # Creating input node\n    in_fields = [\"stacks\"]\n    if load_masks:\n        in_fields += [\"masks\"]\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=in_fields), name=\"inputnode\"\n    )\n\n    enabled_cropping = (\n        False if cfg.reconstruction.pipeline == \"svrtk\" else True\n    )\n    prepro_pipe = get_prepro(cfg, load_masks, enabled_cropping)\n    recon = get_recon(cfg)\n\n    rec_pipe.connect(inputnode, \"stacks\", prepro_pipe, \"inputnode.stacks\")\n    if load_masks:\n        rec_pipe.connect(inputnode, \"masks\", prepro_pipe, \"inputnode.masks\")\n    # RECONSTRUCTION\n\n    rec_pipe.connect(\n        prepro_pipe, \"outputnode.stacks\", recon, \"inputnode.stacks\"\n    )\n    rec_pipe.connect(prepro_pipe, \"outputnode.masks\", recon, \"inputnode.masks\")\n\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"output_srr\", \"output_seg\"]),\n        name=\"outputnode\",\n    )\n    rec_pipe.connect(recon, \"outputnode.srr_volume\", outputnode, \"output_srr\")\n\n    return rec_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.create_seg_pipeline","title":"<code>create_seg_pipeline(cfg, name='seg_pipeline')</code>","text":"<p>Create the segmentation workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <code>name</code> <p>Name of the pipeline (default = \"seg_pipeline\").</p> <code>'seg_pipeline'</code> <p>Returns:     seg_pipe: A Nipype workflow object that contains the segmentation steps</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def create_seg_pipeline(cfg, name=\"seg_pipeline\"):\n    \"\"\"\n    Create the segmentation workflow based on the pipeline specified\n    in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n        name: Name of the pipeline (default = \"seg_pipeline\").\n    Returns:\n        seg_pipe: A Nipype workflow object that contains the segmentation steps\n    \"\"\"\n    print(\"Full pipeline name: \", name)\n    # Creating pipeline\n    seg_pipe = pe.Workflow(name=name)\n    config.update_config(seg_pipe.config)\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"srr_volume\"]), name=\"inputnode\"\n    )\n\n    segmentation = get_seg(cfg)\n\n    seg_pipe.connect(\n        inputnode, \"srr_volume\", segmentation, \"inputnode.srr_volume\"\n    )\n\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"output_seg\"]),\n        name=\"outputnode\",\n    )\n    seg_pipe.connect(\n        segmentation, \"outputnode.seg_volume\", outputnode, \"output_seg\"\n    )\n\n    return seg_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.create_surf_pipeline","title":"<code>create_surf_pipeline(cfg, name='surf_pipeline')</code>","text":"<p>Create the surface extraction workflow based on the pipeline specified in the config <code>cfg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration object containing the parameters for the pipeline.</p> required <code>name</code> <p>Name of the pipeline (default = \"surf_pipeline\").</p> <code>'surf_pipeline'</code> <p>Returns:     surf_pipe: A Nipype workflow object that contains the         surface extraction steps</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def create_surf_pipeline(cfg, name=\"surf_pipeline\"):\n    \"\"\"\n    Create the surface extraction workflow based on the pipeline\n    specified in the config `cfg`.\n\n    Args:\n        cfg: Configuration object containing the parameters for the pipeline.\n        name: Name of the pipeline (default = \"surf_pipeline\").\n    Returns:\n        surf_pipe: A Nipype workflow object that contains the\n            surface extraction steps\n    \"\"\"\n    print(\"Full pipeline name: \", name)\n    # Creating pipeline\n    surf_pipe = pe.Workflow(name=name)\n    config.update_config(surf_pipe.config)\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"seg_volume\"]), name=\"inputnode\"\n    )\n\n    surface = get_surf(cfg)\n\n    surf_pipe.connect(inputnode, \"seg_volume\", surface, \"inputnode.seg_volume\")\n\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"output_surf_lh\", \"output_surf_rh\"]),\n        name=\"outputnode\",\n    )\n    surf_pipe.connect(\n        surface, \"outputnode.surf_volume_lh\", outputnode, \"output_surf_lh\"\n    )\n\n    surf_pipe.connect(\n        surface, \"outputnode.surf_volume_rh\", outputnode, \"output_surf_rh\"\n    )\n    return surf_pipe\n</code></pre>"},{"location":"api_pipelines/#fetpype.pipelines.full_pipeline.create_dhcp_subpipe","title":"<code>create_dhcp_subpipe(name='dhcp_pipe', params={})</code>","text":"<p>Deprecated: Create a dHCP pipeline for fetal brain segmentation and surface extraction.</p> Source code in <code>fetpype/pipelines/full_pipeline.py</code> <pre><code>def create_dhcp_subpipe(name=\"dhcp_pipe\", params={}):\n    \"\"\"\n    Deprecated: Create a dHCP pipeline for fetal brain segmentation\n    and surface extraction.\n    \"\"\"\n\n    print(\"Full pipeline name: \", name)\n\n    # Creating pipeline\n    full_fet_pipe = pe.Workflow(name=name)\n\n    # Creating input node\n    inputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"T2\", \"mask\", \"gestational_age\"]),\n        name=\"inputnode\",\n    )\n\n    # Check params to see if we need to run the seg or surf part, or both.\n    # Params to look is [dhcp][seg] and [dhcp][surf]\n    flag = None\n    if \"dhcp\" in params.keys():\n        if params[\"dhcp\"][\"surf\"] and params[\"dhcp\"][\"seg\"]:\n            flag = \"-all\"\n        elif params[\"dhcp\"][\"seg\"]:\n            flag = \"-seg\"\n        elif params[\"dhcp\"][\"surf\"]:\n            flag = \"-surf\"\n\n    else:\n        print(\"No dhcp parameters found, running both seg and surf\")\n        flag = \"-all\"\n\n    # PREPROCESSING\n    # 1. Run the dhcp pipeline for segmentation\n    dhcp_seg = pe.Node(\n        interface=niu.Function(\n            input_names=[\n                \"T2\",\n                \"mask\",\n                \"gestational_age\",\n                \"pre_command\",\n                \"dhcp_image\",\n                \"threads\",\n                \"flag\",\n            ],\n            output_names=[\"dhcp_files\"],\n            function=dhcp_pipeline,\n        ),\n        name=\"dhcp_seg\",\n    )\n\n    if \"general\" in params.keys():\n        dhcp_seg.inputs.pre_command = params[\"general\"].get(\"pre_command\", \"\")\n        dhcp_seg.inputs.dhcp_image = params[\"general\"].get(\"dhcp_image\", \"\")\n\n    if \"dhcp\" in params.keys():\n        dhcp_seg.inputs.threads = params[\"dhcp\"].get(\"threads\", \"\")\n        dhcp_seg.inputs.flag = flag\n\n    full_fet_pipe.connect(inputnode, \"T2\", dhcp_seg, \"T2\")\n    full_fet_pipe.connect(inputnode, \"mask\", dhcp_seg, \"mask\")\n\n    full_fet_pipe.connect(\n        inputnode, \"gestational_age\", dhcp_seg, \"gestational_age\"\n    )\n\n    # OUTPUT\n    outputnode = pe.Node(\n        niu.IdentityInterface(fields=[\"dhcp_files\"]), name=\"outputnode\"\n    )\n\n    full_fet_pipe.connect(dhcp_seg, \"dhcp_files\", outputnode, \"dhcp_files\")\n\n    return full_fet_pipe\n</code></pre>"},{"location":"api_utils/","title":"Utils","text":""},{"location":"api_utils/#fetpype.utils.utils_bids","title":"<code>utils_bids</code>","text":""},{"location":"api_utils/#fetpype.utils.utils_bids.create_datasource","title":"<code>create_datasource(output_query, data_dir, nipype_dir, subjects=None, sessions=None, acquisitions=None, derivative=None, name='bids_datasource', extra_derivatives=None, save_db=False)</code>","text":"<p>Create a datasource node that have iterables following BIDS format. By default, from a BIDSLayout, lists all the subjects (<code>&lt;sub&gt;</code>), finds their session numbers (<code>&lt;ses&gt;</code>, if any) and their acquisition type (<code>&lt;acq&gt;</code>, if any), and builds an iterable of tuples (sub, ses, acq) with all valid combinations.</p> <p>If a list of subjects/sessions/acquisitions is provided, the BIDSLayout is not queried and the provided subjects/sessions/acquisitions are used as is.</p> <p>If derivative is not None, the BIDSLayout will be queried for the specified derivative.</p> <p>For example, if provided with subjects=[\"sub01\", \"sub02\"], sessions=[\"01\"], acq=[\"haste\", \"tru\"], the datagrabber will attempt at retrieving all of the following combinations: <pre><code>[(\"sub01\", \"01\", \"haste\"), (\"sub01\", \"01\",\"tru\"),\n (\"sub02\", \"01\", \"haste\"), (\"sub02\", \"01\",\"tru\")]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>output_query</code> <code>dict</code> <p>A dictionary specifying the output query for the BIDSDataGrabber.</p> required <code>data_dir</code> <code>str</code> <p>The base directory of the BIDS dataset.</p> required <code>subjects</code> <code>list</code> <p>List of subject IDs to include. If None, all subjects in the dataset are included.</p> <code>None</code> <code>sessions</code> <code>list</code> <p>List of session IDs to include. If None, all sessions for each subject are included.</p> <code>None</code> <code>acquisitions</code> <code>list</code> <p>List of acquisition types to include. If None, all acquisitions for each subject/session are included.</p> <code>None</code> <code>derivative</code> <code>str</code> <p>The name of the derivative to query. If None, no derivative is queried.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name for the datasource node. Defaults to                 \"bids_datasource\".</p> <code>'bids_datasource'</code> <code>extra_derivatives</code> <code>list or str</code> <p>Additional derivatives to include. If provided, these will be added to the BIDSDataGrabber.</p> <code>None</code> <p>Returns:     pe.Node: A configured BIDSDataGrabber node that retrieves data     according to the specified parameters.</p> Source code in <code>fetpype/utils/utils_bids.py</code> <pre><code>def create_datasource(\n    output_query,\n    data_dir,\n    nipype_dir,\n    subjects=None,\n    sessions=None,\n    acquisitions=None,\n    derivative=None,\n    name=\"bids_datasource\",\n    extra_derivatives=None,\n    save_db=False,\n):\n    \"\"\"Create a datasource node that have iterables following BIDS format.\n    By default, from a BIDSLayout, lists all the subjects (`&lt;sub&gt;`),\n    finds their session numbers (`&lt;ses&gt;`, if any) and their acquisition\n    type (`&lt;acq&gt;`, if any), and builds an iterable of tuples\n    (sub, ses, acq) with all valid combinations.\n\n    If a list of subjects/sessions/acquisitions is provided, the\n    BIDSLayout is not queried and the provided\n    subjects/sessions/acquisitions are used as is.\n\n    If derivative is not None, the BIDSLayout will be queried for\n    the specified derivative.\n\n    For example, if provided with subjects=[\"sub01\", \"sub02\"],\n    sessions=[\"01\"], acq=[\"haste\", \"tru\"], the datagrabber will\n    attempt at retrieving all of the following combinations:\n    ```\n    [(\"sub01\", \"01\", \"haste\"), (\"sub01\", \"01\",\"tru\"),\n     (\"sub02\", \"01\", \"haste\"), (\"sub02\", \"01\",\"tru\")]\n    ```\n\n    Args:\n        output_query (dict): A dictionary specifying the output query\n            for the BIDSDataGrabber.\n        data_dir (str): The base directory of the BIDS dataset.\n        subjects (list, optional): List of subject IDs to include.\n            If None, all subjects in the dataset are included.\n        sessions (list, optional): List of session IDs to include.\n            If None, all sessions for each subject are included.\n        acquisitions (list, optional): List of acquisition types to include.\n            If None, all acquisitions for each subject/session are included.\n        derivative (str, optional): The name of the derivative to query.\n            If None, no derivative is queried.\n        name (str, optional): Name for the datasource node. Defaults to\n                            \"bids_datasource\".\n        extra_derivatives (list or str, optional): Additional\n            derivatives to include. If provided, these will be\n            added to the BIDSDataGrabber.\n    Returns:\n        pe.Node: A configured BIDSDataGrabber node that retrieves data\n        according to the specified parameters.\n    \"\"\"\n\n    bids_datasource = pe.Node(\n        interface=nio.BIDSDataGrabber(),\n        name=name,\n        synchronize=True,\n    )\n\n    bids_datasource.inputs.base_dir = data_dir\n    if extra_derivatives is not None:\n        bids_datasource.inputs.index_derivatives = True\n        if isinstance(extra_derivatives, str):\n            extra_derivatives = [extra_derivatives]\n        bids_datasource.inputs.extra_derivatives = extra_derivatives\n    bids_datasource.inputs.output_query = output_query\n\n    layout = BIDSLayout(data_dir, validate=False)\n\n    # Needed as the layout uses validate which\n    # does not work with our segmentation files.\n\n    if save_db:\n        layout_db = os.path.join(nipype_dir, \"layout_db\")\n        if os.path.exists(layout_db):\n            os.remove(os.path.join(layout_db, \"layout_index.sqlite\"))\n        layout.save(layout_db)\n\n        bids_datasource.inputs.load_layout = layout_db\n\n    # Verbose\n    print(\"BIDS layout:\", layout)\n    print(\"\\t\", layout.get_subjects())\n    print(\"\\t\", layout.get_sessions())\n    iterables = [(\"subject\", \"session\", \"acquisition\"), []]\n\n    existing_sub = layout.get_subjects()\n    if subjects is None:\n        subjects = existing_sub\n\n    for sub in subjects:\n        if sub not in existing_sub:\n            raise ValueError(\n                f\"Requested subject {sub} was not found in the \"\n                f\"folder {data_dir}.\"\n            )\n\n        existing_ses = layout.get_sessions(subject=sub)\n        if sessions is None:\n            sessions_subj = existing_ses\n        else:\n            sessions_subj = sessions\n        # If no sessions are found, it is possible that there is no session.\n        sessions_subj = [None] if len(sessions_subj) == 0 else sessions_subj\n        for ses in sessions_subj:\n            if ses is not None and ses not in existing_ses:\n                print(\n                    f\"WARNING: Session {ses} was not found for subject {sub}.\"\n                )\n            existing_acq = layout.get_acquisition(subject=sub, session=ses)\n            if acquisitions is None:\n                acquisitions_subj = existing_acq\n            else:\n                acquisitions_subj = acquisitions\n            # If there is no acquisition found, maybe the acquisition\n            # tag was not specified.\n            acquisitions_subj = (\n                [None] if len(acquisitions_subj) == 0 else acquisitions_subj\n            )\n            for acq in acquisitions_subj:\n                if acq is not None and acq not in existing_acq:\n                    print(\n                        f\"WARNING: Acquisition {acq} was not found for \"\n                        f\"subject {sub} session {ses}.\"\n                    )\n\n                iterables[1] += [(sub, ses, acq)]\n\n    bids_datasource.iterables = iterables\n\n    return bids_datasource\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_bids.create_bids_datasink","title":"<code>create_bids_datasink(out_dir, pipeline_name, strip_dir, datatype='anat', name=None, rec_label=None, seg_label=None, surf_label=None, desc_label=None, custom_subs=None, custom_regex_subs=None)</code>","text":"<p>Creates a BIDS-compatible datasink using parameterization and regex substitutions. Organizes outputs into: /derivatives//sub-/[ses-/] / Source code in <code>fetpype/utils/utils_bids.py</code> <pre><code>def create_bids_datasink(\n    out_dir,\n    pipeline_name,\n    strip_dir,\n    datatype=\"anat\",\n    name=None,\n    rec_label=None,\n    seg_label=None,\n    surf_label=None,\n    desc_label=None,\n    custom_subs=None,\n    custom_regex_subs=None,\n):\n    \"\"\"\n    Creates a BIDS-compatible datasink using parameterization and\n    regex substitutions.\n    Organizes outputs into:\n    &lt;out_dir&gt;/derivatives/&lt;pipeline_name&gt;/sub-&lt;ID&gt;/[ses-&lt;ID&gt;/]\n    &lt;datatype&gt;/&lt;BIDS_filename&gt;\n    \"\"\"\n    if not strip_dir:\n        raise ValueError(\n            \"`strip_dir` (Nipype work dir base path) is required.\"\n        )\n    if name is None:\n        name = f\"{pipeline_name}_datasink\"\n\n    datasink = pe.Node(\n        nio.DataSink(\n            base_directory=out_dir, parameterization=True, strip_dir=strip_dir\n        ),\n        name=name,\n    )\n\n    regex_subs = []\n\n    bids_derivatives_root = out_dir  # we already pass the bids derivatives\n    escaped_bids_derivatives_root = re.escape(out_dir)\n\n    if pipeline_name == \"preprocessing\":\n        # ** Rule 1: Preprocessing Stacks (Denoised) **\n        if desc_label == \"denoised\":\n            # with session\n            regex_subs.append(\n                (\n                    (\n                        rf\"^{escaped_bids_derivatives_root}/\"\n                        rf\".*?_?session_([^/]+)\"\n                        rf\"_subject_([^/]+).*?/?_denoising.*/\"\n                        rf\"(sub-[^_]+_ses-[^_]+(?:_run-\\d+))?\"\n                        rf\"_T2w_noise_corrected(\\.nii(?:\\.gz)?)$\"\n                    ),\n                    (\n                        rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/{datatype}\"\n                        rf\"/\\3_desc-denoised_T2w\\4\"\n                    ),\n                )\n            )\n            # without session\n            regex_subs.append(\n                (\n                    (\n                        rf\"^{escaped_bids_derivatives_root}/\"\n                        rf\"(?!.*?_?session_[^/]+).*?_?subject_([^/]+).*?/\"\n                        rf\"?_denoising.*/\"\n                        rf\"(sub-[^_]+(?:_run-\\d+)?)?_?\"\n                        rf\"T2w_noise_corrected(\\.nii(?:\\.gz)?)$\"\n                    ),\n                    (\n                        rf\"{bids_derivatives_root}/sub-\\1/{datatype}\"\n                        rf\"/\\2_desc-denoised_T2w\\3\"\n                    ),\n                )\n            )\n\n        # ** Rule 2: Preprocessing Masks (Cropped) **\n        if desc_label == \"cropped\":\n            # with session\n            regex_subs.append(\n                (\n                    (\n                        rf\"^{escaped_bids_derivatives_root}/.*?_session_\"\n                        rf\"([^/]+)_subject_([^/]+).*/?_cropping.*/\"\n                        rf\"(sub-[^_]+_ses-[^_]+(?:_run-\\d+)?)_\"\n                        rf\"mask(\\.nii(?:\\.gz)?)$\"\n                    ),\n                    (\n                        rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/{datatype}\"\n                        rf\"/\\3_desc-cropped_mask\\4\"\n                    ),\n                )\n            )\n            # without session\n            regex_subs.append(\n                (\n                    (\n                        rf\"^{escaped_bids_derivatives_root}/\"\n                        rf\"(?!.*?_session_[^/]+).*?_subject_\"\n                        rf\"([^/]+).*/?_cropping.*/\"\n                        rf\"(sub-[^_]+(?:_run-\\d+)?)_mask(\\.nii(?:\\.gz)?)$\"\n                    ),\n                    (\n                        rf\"{bids_derivatives_root}/sub-\\1/{datatype}\"\n                        rf\"/\\2_desc-cropped_mask\\3\"\n                    ),\n                )\n            )\n\n    # ** Rule 3: Reconstruction Output **\n    if rec_label and not seg_label and pipeline_name != \"preprocessing\":\n        # with session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/.*?_?session_([^/]+)\"\n                    rf\"_subject_([^/]+).*/(?:[^/]+)(\\.nii(?:\\.gz)?)$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/\"\n                    rf\"{datatype}/sub-\\2_ses-\\1_rec-{rec_label}_T2w\\3\"\n                ),\n            )\n        )\n        # without session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/(?!.*?_?session_[^/]+)\"\n                    rf\".*?_?subject_\"\n                    rf\"([^/]+).*/(?:[^/]+)(\\.nii(?:\\.gz)?)$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\1/\"\n                    rf\"{datatype}/sub-\\1_rec-{rec_label}_T2w\\2\"\n                ),\n            )\n        )\n\n    # ** Rule 4: Segmentation Output **\n    if seg_label and rec_label and pipeline_name != \"preprocessing\":\n        # with session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/\"\n                    rf\".*?_?session_([^/]+)_subject_([^/]+).*/\"\n                    rf\"input_srr-mask-brain_bounti-19(\\.nii(?:\\.gz)?)$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/{datatype}/\"\n                    rf\"sub-\\2_ses-\\1_rec-{rec_label}_seg-{seg_label}_dseg\\3\"\n                ),\n            )\n        )\n        # without session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/(?!.*?_?session_[^/]+)\"\n                    rf\".*?_?subject_([^/]+).*/\"\n                    rf\"input_srr-mask-brain_bounti-19(\\.nii(?:\\.gz)?)$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\1/{datatype}/\"\n                    rf\"sub-\\1_rec-{rec_label}_seg-{seg_label}_dseg\\2\"\n                ),\n            )\n        )\n\n    # ** Rule 5: Surface outputs (.gii) **\n    if surf_label:\n        label = f\"_rec-{rec_label}\" if rec_label else \"\"\n        label += f\"_seg-{seg_label}\" if seg_label else \"\"\n        # with session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/\"\n                    rf\".*?_?session_([^/]+)_subject_([^/]+).*/\"\n                    rf\"([^/]+)\\.gii$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/{datatype}/\"\n                    rf\"sub-\\2_ses-\\1{label}_\\3.gii\"\n                ),\n            )\n        )\n        # without session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/(?!.*?_?session_[^/]+)\"\n                    rf\".*?_?subject_([^/]+).*/\"\n                    rf\"([^/]+)\\.gii$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\1/{datatype}/\"\n                    rf\"sub-\\1{label}_\\2.gii\"\n                ),\n            )\n        )\n\n        # ** Rule 6: Surface outputs (.stl) **\n        # with session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/\"\n                    rf\".*?_?session_([^/]+)_subject_([^/]+).*/\"\n                    rf\"([^/]+)\\.stl$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\2/ses-\\1/{datatype}/\"\n                    rf\"sub-\\2_ses-\\1{label}_\\3.stl\"\n                ),\n            )\n        )\n        # without session\n        regex_subs.append(\n            (\n                (\n                    rf\"^{escaped_bids_derivatives_root}/(?!.*?_?session_[^/]+)\"\n                    rf\".*?_?subject_([^/]+).*/\"\n                    rf\"([^/]+)\\.stl$\"\n                ),\n                (\n                    rf\"{bids_derivatives_root}/sub-\\1/{datatype}/\"\n                    rf\"sub-\\1{label}_\\2.stl\"\n                ),\n            )\n        )\n\n    # --- Generic cleanup rules (unchanged, but keep them\n    # after the mapping rules) ---\n    regex_subs.extend(\n        [\n            (r\"sub-sub-\", r\"sub-\"),\n            (r\"ses-ses-\", r\"ses-\"),\n            (r\"_+\", \"_\"),\n            (r\"(/)_\", r\"\\1\"),\n            (r\"(_)\\.\", r\"\\.\"),\n            (r\"-+\", \"-\"),\n            (r\"//+\", \"/\"),\n            (r\"[\\\\/]$\", \"\"),\n            (r\"_ses-None\", \"\"),  # in case something injected a None\n            (r\"(\\.nii\\.gz)\\1+$\", r\"\\1\"),\n            (r\"(\\.nii)\\1+$\", r\"\\1\"),\n        ]\n    )\n\n    # Add custom regex substitutions\n    if custom_regex_subs:\n        regex_subs.extend(custom_regex_subs)\n\n    datasink.inputs.regexp_substitutions = regex_subs\n\n    # Add custom simple substitutions\n    final_subs = []\n    if custom_subs:\n        final_subs.extend(custom_subs)\n    datasink.inputs.substitutions = final_subs\n\n    return datasink\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_bids.create_datasink","title":"<code>create_datasink(iterables, name='output', params_subs={}, params_regex_subs={})</code>","text":"<p>Deprecated. Creates a data sink node for reformatting and organizing relevant outputs.</p> <p>From: https://github.com/Macatools/macapype (adapted)</p> <p>Parameters:</p> Name Type Description Default <code>iterables</code> <code>list or tuple</code> <p>A collection of iterables, containing                        subject and session information.</p> required <code>name</code> <code>str</code> <p>The name for the data sink container.                   Defaults to \"output\".</p> <code>'output'</code> <code>params_subs</code> <code>dict</code> <p>A dictionary of parameter substitutions                           to apply to output paths. Defaults to                           an empty dictionary.</p> <code>{}</code> <code>params_regex_subs</code> <code>dict</code> <p>A dictionary of regular                                 expression-based substitutions                                 to apply to output paths.                                 Defaults to an empty dictionary.</p> <code>{}</code> <p>Returns:</p> Type Description <p>pe.Node: A Pipeline Engine Node representing the configured datasink.</p> Source code in <code>fetpype/utils/utils_bids.py</code> <pre><code>def create_datasink(\n    iterables, name=\"output\", params_subs={}, params_regex_subs={}\n):\n    \"\"\"\n    Deprecated. Creates a data sink node for reformatting and organizing\n    relevant outputs.\n\n    From: https://github.com/Macatools/macapype (adapted)\n\n    Args:\n        iterables (list or tuple): A collection of iterables, containing\n                                   subject and session information.\n        name (str, optional): The name for the data sink container.\n                              Defaults to \"output\".\n        params_subs (dict, optional): A dictionary of parameter substitutions\n                                      to apply to output paths. Defaults to\n                                      an empty dictionary.\n        params_regex_subs (dict, optional): A dictionary of regular\n                                            expression-based substitutions\n                                            to apply to output paths.\n                                            Defaults to an empty dictionary.\n\n    Returns:\n        pe.Node: A Pipeline Engine Node representing the configured datasink.\n    \"\"\"\n\n    print(\"Datasink name: \", name)\n\n    # Create the datasink node\n    datasink = pe.Node(nio.DataSink(), name=name)\n\n    # Generate subject folders with session and subject information\n    subjFolders = [\n        (\n            \"_acquisition_%s_session_%s_subject_%s\" % (acq, ses, sub),\n            \"sub-%s/ses-%s/anat\" % (sub, ses),\n        )\n        for (sub, ses, acq) in iterables[1]  # doublecheck\n    ]\n\n    print(\"subjFolders: \", subjFolders)\n\n    # Load parameter substitutions from the 'subs.json' file\n    json_subs = op.join(op.dirname(op.abspath(__file__)), \"subs.json\")\n    dict_subs = json.load(open(json_subs, encoding=\"utf-8\"))\n    dict_subs.update(params_subs)  # Override with any provided substitutions\n\n    subs = [(key, value) for key, value in dict_subs.items()]\n    subjFolders.extend(subs)  # Add parameter substitutions to folders\n\n    datasink.inputs.substitutions = subjFolders\n\n    # Load regex-based substitutions from the 'regex_subs.json' file\n    json_regex_subs = op.join(\n        op.dirname(op.abspath(__file__)), \"regex_subs.json\"\n    )\n    dict_regex_subs = json.load(open(json_regex_subs, encoding=\"utf-8\"))\n\n    # Update with provided regex substitutions\n    dict_regex_subs.update(params_regex_subs)\n\n    regex_subs = [(key, value) for key, value in dict_regex_subs.items()]\n    datasink.inputs.regexp_substitutions = regex_subs\n\n    return datasink\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_bids.get_gestational_age","title":"<code>get_gestational_age(bids_dir, T2)</code>","text":"<p>Retrieve the gestational age for a specific subject from a BIDS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bids_dir </code> <p>The file path to the root of the BIDS dataset, which must contain a 'participants.tsv' file.</p> required <code>T2 </code> <p>The path of the image. We can get the subject id from there if it follows a BIDS format.</p> required <p>Returns:     gestational_age : The gestational age of the subject.</p> Source code in <code>fetpype/utils/utils_bids.py</code> <pre><code>def get_gestational_age(bids_dir, T2):\n    \"\"\"\n    Retrieve the gestational age for a specific subject from a BIDS dataset.\n\n    Args:\n        bids_dir : The file path to the root of the BIDS dataset,\n            which must contain a 'participants.tsv' file.\n        T2 : The path of the image. We can get the subject id from there if\n            it follows a BIDS format.\n    Returns:\n        gestational_age : The gestational age of the subject.\n\n    \"\"\"\n    import pandas as pd\n    import os\n\n    participants_path = f\"{bids_dir}/participants.tsv\"\n\n    try:\n        df = pd.read_csv(participants_path, delimiter=\"\\t\")\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"participants.tsv not found in {bids_dir}\")\n\n    # TODO This T2[0] not really clean\n    subject_id = os.path.basename(T2).split(\"_\")[0]\n    try:\n        gestational_age = df.loc[\n            df[\"participant_id\"] == f\"{subject_id}\", \"gestational_age\"\n        ].values[0]\n    except KeyError:\n        raise KeyError(\n            \"Column 'gestational_age' not found in participants.tsv\"\n        )\n    except IndexError:\n        raise IndexError(\n            f\"Subject sub-{subject_id} not found in participants.tsv\"\n        )\n\n    return gestational_age\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_bids.create_description_file","title":"<code>create_description_file(out_dir, algo, prev_desc=None, cfg=None)</code>","text":"<p>Create a dataset_description.json file in the derivatives folder. TODO: should look for the extra parameters and also add them</p> <p>Parameters:</p> Name Type Description Default <code>args </code> <p>Dictionary containing the arguments passed to the script.</p> required <code>container_type </code> <p>Type of container used to run the algorithm.</p> required Source code in <code>fetpype/utils/utils_bids.py</code> <pre><code>def create_description_file(out_dir, algo, prev_desc=None, cfg=None):\n    \"\"\"Create a dataset_description.json file in the derivatives folder.\n    TODO: should look for the extra parameters and also add them\n\n    Args:\n        args : Dictionary containing the arguments passed to the script.\n        container_type : Type of container used to run the algorithm.\n    \"\"\"\n    if not os.path.exists(os.path.join(out_dir, \"dataset_description.json\")):\n        description = {\n            \"Name\": algo,\n            \"Version\": \"1.0\",\n            \"BIDSVersion\": \"1.7.0\",\n            \"PipelineDescription\": {\n                \"Name\": algo,\n            },\n            \"GeneratedBy\": [\n                {\n                    \"Name\": algo,\n                }\n            ],\n        }\n\n        if prev_desc is not None:\n            with open(prev_desc, \"r\") as f:\n                prev_desc = json.load(f)\n                description[\"GeneratedBy\"].append({\"Name\": prev_desc[\"Name\"]})\n        if cfg is not None:\n            description[\"Config\"] = OmegaConf.to_container(cfg, resolve=True)\n        with open(\n            os.path.join(\n                out_dir,\n                \"dataset_description.json\",\n            ),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as outfile:\n            json.dump(description, outfile, indent=4)\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_docker","title":"<code>utils_docker</code>","text":""},{"location":"api_utils/#fetpype.utils.utils_docker.flatten_cfg","title":"<code>flatten_cfg(cfg, base='')</code>","text":"<p>Flatten a nested configuration dictionary into a flat dictionary with keys as paths and values as the corresponding values. Args:     cfg (dict): The configuration dictionary to flatten.     base (str): The base path to prepend to the keys. Returns:     generator: A generator that yields tuples of (path, value).</p> Source code in <code>fetpype/utils/utils_docker.py</code> <pre><code>def flatten_cfg(cfg, base=\"\"):\n    \"\"\"\n    Flatten a nested configuration dictionary into a flat dictionary\n    with keys as paths and values as the corresponding values.\n    Args:\n        cfg (dict): The configuration dictionary to flatten.\n        base (str): The base path to prepend to the keys.\n    Returns:\n        generator: A generator that yields tuples of (path, value).\n    \"\"\"\n    for k, v in cfg.items():\n        if isinstance(v, dict):\n            yield from flatten_cfg(v, \"/\".join([base, k]))\n        else:\n            yield (\"/\".join([base, k]), v)\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_docker.is_available_container","title":"<code>is_available_container(container_type, container_name)</code>","text":"<p>Check if the container is available on the system. Args:     container_type (str):   The type of container, either 'docker'                             or 'singularity'     container_name (str): The name of the container to check. Returns:     bool: True if the container is available, False otherwise.</p> Source code in <code>fetpype/utils/utils_docker.py</code> <pre><code>def is_available_container(container_type, container_name):\n    \"\"\"\n    Check if the container is available on the system.\n    Args:\n        container_type (str):   The type of container, either 'docker'\n                                or 'singularity'\n        container_name (str): The name of the container to check.\n    Returns:\n        bool: True if the container is available, False otherwise.\n    \"\"\"\n    if container_type == \"docker\":\n        try:\n            subprocess.run(\n                [container_type, \"inspect\", container_name],\n                check=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n        except subprocess.CalledProcessError:\n            return False\n        else:\n            return True\n    elif container_type == \"singularity\":\n        if os.path.isfile(container_name):\n            return True\n        else:\n            return False\n    else:\n        raise ValueError(\n            f\"Container type {container_type} not supported. \"\n            \"Please use 'docker' or 'singularity'.\"\n        )\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_docker.retrieve_container","title":"<code>retrieve_container(container_type, container_name)</code>","text":"<pre><code>Retrieve the container from the registry.\n</code></pre> <p>Args:     container_type (str):   The type of container, either 'docker' or                             'singularity'     container_name (str): The name of the container to retrieve.</p> Source code in <code>fetpype/utils/utils_docker.py</code> <pre><code>def retrieve_container(container_type, container_name):\n    \"\"\"\n        Retrieve the container from the registry.\n    Args:\n        container_type (str):   The type of container, either 'docker' or\n                                'singularity'\n        container_name (str): The name of the container to retrieve.\n\n    \"\"\"\n    if container_type == \"docker\":\n\n        cmd = [container_type, \"pull\", container_name]\n        print(f\"Running {' '.join(cmd)}\")\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        for line in process.stdout:\n            print(line, end=\"\")\n\n        process.wait()\n        if process.returncode != 0:\n            for line in process.stderr:\n                print(line, end=\"\")\n            raise subprocess.CalledProcessError(\n                process.returncode,\n                cmd,\n                output=process.stdout.read(),\n                stderr=process.stderr.read(),\n            )\n    elif container_type == \"singularity\":\n        raise NotImplementedError\n    else:\n        raise ValueError(\n            f\"Container type {container_type} not supported. \"\n            \"Please use 'docker' or 'singularity'.\"\n        )\n</code></pre>"},{"location":"api_utils/#fetpype.utils.utils_docker.check_container_commands","title":"<code>check_container_commands(container_type, cfg)</code>","text":"<p>Check if the required docker or singularity images are available on the system.</p> <p>Parameters:</p> Name Type Description Default <code>container_type</code> <code>str</code> <p>The type of container, either 'docker' or                     'singularity'</p> required <code>cfg</code> <code>dict</code> <p>The configuration dictionary containing the         container names.</p> required Source code in <code>fetpype/utils/utils_docker.py</code> <pre><code>def check_container_commands(container_type, cfg):\n    \"\"\"\n    Check if the required docker or singularity images are available\n    on the system.\n\n    Args:\n        container_type (str):   The type of container, either 'docker' or\n                                'singularity'\n        cfg (dict): The configuration dictionary containing the\n                    container names.\n\n    \"\"\"\n    # Check if the container_type is valid\n    if container_type not in [\"docker\", \"singularity\"]:\n        raise ValueError(\n            f\"Container type {container_type} not supported. \"\n            \"Please use 'docker' or 'singularity'.\"\n        )\n\n    # Iterate the nested config dictionary\n    cfg_dict = dict(flatten_cfg(cfg))\n    container_names = {}\n    for k, v in cfg_dict.items():\n        # Return the word that is after the &lt;mount&gt; tag in the string v\n        if container_type == \"docker\" and \"docker\" in k:\n            docker_name = v.split(\"&lt;mount&gt;\")[-1].split()[0]\n            container_names[k] = docker_name\n        elif container_type == \"singularity\" and \"singularity\" in k:\n            # Find a string that ends with .sif\n            if v is None:\n                continue\n            print(\"CHECKING PATH IN\", k, v)\n            singularity_name = [s for s in v.split(\" \") if s.endswith(\".sif\")][\n                0\n            ]\n            container_names[k] = singularity_name\n\n    container_names_list = defaultdict(list)\n    for k, v in container_names.items():\n        container_names_list[v].append(k)\n\n    # Check which containers are missing\n    missing_containers = []\n    for k, v in container_names_list.items():\n        print(f\"Checking {container_type} {k} -- Used by {', '.join(v)}\")\n        if not is_available_container(container_type, k):\n            print(\"\\tSTATUS: NOT FOUND\")\n            missing_containers.append(k)\n        else:\n            print(\"\\tSTATUS: AVAILABLE\")\n\n    # Retrieve the missing containers\n    if len(missing_containers) &gt; 0 and container_type == \"docker\":\n        var = input(\n            f\"Would you like me to retrieve the missing containers \"\n            f\"{', '.join(missing_containers)}? (y/n) \"\n        )\n        if var == \"y\":\n            for k in missing_containers:\n                retrieve_container(container_type, k)\n\n        else:\n            print(\"Exiting...\")\n            sys.exit(1)\n    elif len(missing_containers) &gt; 0 and container_type == \"singularity\":\n        raise NotImplementedError(\n            \"Automated container retrieval for singularity is \"\n            \"not implemented yet.\"\n        )\n        sys.exit(1)\n</code></pre>"},{"location":"api_utils/#fetpype.utils.logging","title":"<code>logging</code>","text":""},{"location":"api_utils/#fetpype.utils.logging.StdToLogger","title":"<code>StdToLogger</code>","text":"<p>Redirect a stream (stdout/stderr) into a logger (line-buffered).</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The logger to which the stream will be redirected.</p> required <code>level</code> <code>int</code> <p>The logging level (e.g., logging.INFO, logging.ERROR).</p> required Source code in <code>fetpype/utils/logging.py</code> <pre><code>class StdToLogger:\n    \"\"\"Redirect a stream (stdout/stderr) into a logger (line-buffered).\n\n    Args:\n        logger (logging.Logger): The logger to which the stream will\n            be redirected.\n        level (int): The logging level (e.g., logging.INFO, logging.ERROR).\n\n    \"\"\"\n\n    def __init__(self, logger, level):\n        self.logger = logger\n        self.level = level\n        self._buf = \"\"\n\n    def write(self, s):\n        if not s:\n            return\n        s = s.replace(\"\\r\\n\", \"\\n\")\n        parts = (self._buf + s).split(\"\\n\")\n        for line in parts[:-1]:\n            if line:\n                self.logger.log(self.level, line)\n        self._buf = parts[-1]\n\n    def flush(self):\n        if self._buf:\n            self.logger.log(self.level, self._buf)\n            self._buf = \"\"\n</code></pre>"},{"location":"api_utils/#fetpype.utils.logging.setup_logging","title":"<code>setup_logging(base_dir, debug=False, verbose=False, capture_prints=True, container_logger_name='nipype.container')</code>","text":"<p>Set up logging for the Nipype workflow. Ensures the possibility of limited console output while providing detailed logging to a file. Args:     base_dir (str): The base directory for the workflow.     debug (bool): Enable debug logging.         Logging to  will be at the DEBUG level.     verbose (bool): Enable verbose logging. Logging level to         console will be set to INFO if <code>debug</code> is <code>False</code> and         to DEBUG if <code>debug</code> is <code>True</code>.     capture_prints (bool): Capture print statements.     container_logger_name (str): The name of the container logger. Source code in <code>fetpype/utils/logging.py</code> <pre><code>def setup_logging(\n    base_dir,\n    debug=False,\n    verbose=False,\n    capture_prints=True,\n    container_logger_name=\"nipype.container\",\n):\n    \"\"\"\n    Set up logging for the Nipype workflow.\n    Ensures the possibility of limited console output\n    while providing detailed logging to a file.\n    Args:\n        base_dir (str): The base directory for the workflow.\n        debug (bool): Enable debug logging.\n            Logging to &lt;log_file&gt; will be at the DEBUG level.\n        verbose (bool): Enable verbose logging. Logging level to\n            console will be set to INFO if `debug` is `False` and\n            to DEBUG if `debug` is `True`.\n        capture_prints (bool): Capture print statements.\n        container_logger_name (str): The name of the container logger.\n    \"\"\"\n    log_dir = os.path.join(base_dir, \"logs\", time.strftime(\"%Y%m%d-%H%M%S\"))\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, \"pypeline.log\")\n    try:\n        os.remove(log_file)  # start fresh each run\n    except FileNotFoundError:\n        pass\n\n    file_level = \"DEBUG\" if debug else \"INFO\"\n\n    # Effective console level when verbose\n    to_console = (\n        \"DEBUG\" if (verbose and debug) else \"INFO\" if verbose else \"ERROR\"\n    )\n\n    # Tell Nipype to log to file\n    config.update_config(\n        {\n            \"logging\": {\n                \"log_to_file\": True,\n                \"log_directory\": log_dir,\n                \"log_size\": str(50 * 1024 * 1024),\n                \"log_rotate\": \"5\",\n                \"workflow_level\": file_level,\n                \"interface_level\": file_level,\n                \"utils_level\": file_level,\n            },\n            \"execution\": {\n                \"crashdump_dir\": log_dir,  # \u2190 fixed key name\n                \"crashfile_format\": \"txt\",\n            },\n        }\n    )\n    nlogging.update_logging(config)\n\n    # Parent 'nipype' console handler \u2192 quiet or verbose\n    nipype_root = logging.getLogger(\"nipype\")\n    for h in list(nipype_root.handlers):\n        if isinstance(h, logging.StreamHandler) and not isinstance(\n            h, logging.FileHandler\n        ):\n            h.setLevel(getattr(logging, to_console))\n            h.setFormatter(logging.Formatter(\"%(message)s\"))\n            h.stream = sys.__stdout__\n\n    # Plain-lines logger to the SAME file (no header per line)\n    container_log = logging.getLogger(container_logger_name)\n    container_log.setLevel(getattr(logging, file_level))\n    container_log.propagate = False  # don't bubble into 'nipype' handlers\n\n    # Ensure we have exactly one rotating file handler to pypeline.log\n    if not any(\n        isinstance(h, RFH) and getattr(h, \"baseFilename\", None) == log_file\n        for h in container_log.handlers\n    ):\n        ch = RFH(log_file, maxBytes=50 * 1024 * 1024, backupCount=5)\n        ch.setLevel(getattr(logging, file_level))\n        ch.setFormatter(logging.Formatter(\"%(message)s\"))  # no header\n        container_log.addHandler(ch)\n\n    # In verbose mode, also mirror container lines to the console\n    if verbose and not any(\n        isinstance(h, logging.StreamHandler) for h in container_log.handlers\n    ):\n        sh = logging.StreamHandler(stream=sys.__stdout__)\n        sh.setLevel(getattr(logging, to_console))  # INFO or DEBUG\n        sh.setFormatter(logging.Formatter(\"%(message)s\"))\n        container_log.addHandler(sh)\n\n    # Route print()/tracebacks to the Nipype workflow logger (file) if desired\n    if capture_prints:\n        nipype_workflow_log = logging.getLogger(\"nipype.workflow\")\n        nipype_workflow_log.propagate = True\n        sys.stdout = StdToLogger(nipype_workflow_log, logging.INFO)\n        sys.stderr = StdToLogger(nipype_workflow_log, logging.ERROR)\n\n    # Where Nipype interfaces send their stdout/stderr\n    # - verbose=True  \u2192 stream to terminal\n    # - verbose=False \u2192 save to stdout.nipype / stderr.nipype in node workdirs\n    CommandLine.set_default_terminal_output(\"stream\" if verbose else \"file\")\n\n    logging.captureWarnings(True)\n\n    def _excepthook(exc_type, exc, tb):\n        logging.getLogger(\"nipype.workflow\").error(\n            \"Uncaught exception\", exc_info=(exc_type, exc, tb)\n        )\n        sys.__excepthook__(exc_type, exc, tb)\n\n    sys.excepthook = _excepthook\n</code></pre>"},{"location":"api_utils/#fetpype.utils.logging.run_and_tee","title":"<code>run_and_tee(cmd, *, prefix='')</code>","text":"<p>Run a command, stream output live to terminal, and log every line. Returns the full combined output; raises RuntimeError on non-zero exit.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>The command to run.</p> required <code>prefix</code> <code>str</code> <p>A prefix to add to each line of output.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The combined output of the command.</p> Source code in <code>fetpype/utils/logging.py</code> <pre><code>def run_and_tee(cmd, *, prefix=\"\"):\n    \"\"\"\n    Run a command, stream output live to terminal, and log every line.\n    Returns the full combined output; raises RuntimeError on non-zero exit.\n\n    Args:\n        cmd (str): The command to run.\n        prefix (str): A prefix to add to each line of output.\n\n    Returns:\n        str: The combined output of the command.\n    \"\"\"\n\n    log_plain = logging.getLogger(\"nipype.container\")  # message-only\n    log_evt = logging.getLogger(\"nipype.workflow\")  # structured events\n\n    env = os.environ.copy()\n    env.setdefault(\"PYTHONUNBUFFERED\", \"1\")  # flush Python in container\n    env.setdefault(\"PYTHONIOENCODING\", \"utf-8\")\n    env.setdefault(\"TQDM_DISABLE\", \"1\")  # avoid CR-based progress bars\n\n    proc = subprocess.Popen(\n        cmd,\n        shell=True,  # keep if you're passing a single string\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,  # merge stderr -&gt; stdout\n        text=True,\n        bufsize=1,  # line-buffered\n        env=env,\n    )\n\n    log_evt.info(\"Running: %s\", cmd)  # one structured line\n\n    captured = []\n    for line in proc.stdout:\n        line = line.rstrip(\"\\n\")\n        captured.append(line)\n        log_plain.info(\"%s%s\", prefix, line)  # file: plain line\n        sys.__stdout__.write(prefix + line + \"\\n\")  # console: live\n        sys.__stdout__.flush()\n\n    proc.stdout.close()\n    rc = proc.wait()\n    output = \"\".join(captured)\n\n    if rc != 0:\n        raise RuntimeError(\n            f\"Docker call failed with exit code {rc}.\\n\"\n            f\"Command: {cmd}\\n\"\n            f\"Output:\\n{output.strip() or '&lt;&lt;no output&gt;&gt;'}\"\n        )\n\n    return output\n</code></pre>"},{"location":"api_workflows/","title":"Workflows","text":""},{"location":"api_workflows/#fetpype.workflows.pipeline_fet","title":"<code>pipeline_fet</code>","text":""},{"location":"api_workflows/#fetpype.workflows.pipeline_fet.create_main_workflow","title":"<code>create_main_workflow(data_dir, masks_dir, out_dir, nipype_dir, subjects, sessions, acquisitions, cfg_path, nprocs, save_intermediates=False, debug=False, verbose=False)</code>","text":"<p>Instantiates and runs the entire workflow of the fetpype pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the BIDS directory that contains anatomical images.</p> required <code>out_dir</code> <code>str</code> <p>Path to the output directory (will be created if not already existing). Previous outputs may be overriden.</p> required <code>nipype_dir</code> <code>str</code> <p>Path to the nipype directory.</p> required <code>subjects</code> <code>list[str]</code> <p>List of subject IDs matching the BIDS specification (e.g., sub-[SUB1], sub-[SUB2], ...).</p> required <code>sessions</code> <code>list[str]</code> <p>List of session IDs matching the BIDS specification (e.g., ses-[SES1], ses-[SES2], ...).</p> required <code>acquisitions</code> <code>list[str]</code> <p>List of acquisition names matching the BIDS specification (e.g., acq-[ACQ1], ...).</p> required <code>cfg_path</code> <code>str</code> <p>Path to a hydra  configuration file (YAML) specifying pipeline parameters.</p> required <code>nprocs</code> <code>int</code> <p>Number of processes to be launched by MultiProc.</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug mode.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose mode.</p> <code>False</code> Source code in <code>fetpype/workflows/pipeline_fet.py</code> <pre><code>def create_main_workflow(\n    data_dir,\n    masks_dir,\n    out_dir,\n    nipype_dir,\n    subjects,\n    sessions,\n    acquisitions,\n    cfg_path,\n    nprocs,\n    save_intermediates=False,\n    debug=False,\n    verbose=False,\n):\n    \"\"\"\n    Instantiates and runs the entire workflow of the fetpype pipeline.\n\n    Args:\n        data_dir (str):\n            Path to the BIDS directory that contains anatomical images.\n        out_dir (str):\n            Path to the output directory (will be created if not already\n            existing). Previous outputs may be overriden.\n        nipype_dir (str):\n            Path to the nipype directory.\n        subjects (list[str], optional):\n            List of subject IDs matching the BIDS specification\n            (e.g., sub-[SUB1], sub-[SUB2], ...).\n        sessions (list[str], optional):\n            List of session IDs matching the BIDS specification\n            (e.g., ses-[SES1], ses-[SES2], ...).\n        acquisitions (list[str], optional):\n            List of acquisition names matching the BIDS specification\n            (e.g., acq-[ACQ1], ...).\n        cfg_path (str):\n            Path to a hydra  configuration file (YAML) specifying pipeline\n            parameters.\n        nprocs (int):\n            Number of processes to be launched by MultiProc.\n        debug (bool):\n            Whether to enable debug mode.\n        verbose (bool):\n            Whether to enable verbose mode.\n\n    \"\"\"\n\n    cfg = init_and_load_cfg(cfg_path)\n    pipeline_name = get_pipeline_name(cfg)\n    data_dir, out_dir, nipype_dir = check_and_update_paths(\n        data_dir, out_dir, nipype_dir, pipeline_name\n    )\n\n    setup_logging(\n        base_dir=nipype_dir,\n        debug=debug,\n        verbose=verbose,\n        capture_prints=True,\n    )\n    log = logging.getLogger(\"nipype\")\n    # Print the three paths\n    log.info(\n        f\"Data directory: {data_dir}\\n\"\n        f\"Output directory: {out_dir}\\n\"\n        f\"Nipype directory: {nipype_dir}\"\n    )\n\n    load_masks = False\n    if masks_dir is not None:\n        # Check it exists\n        if not os.path.exists(masks_dir):\n            raise ValueError(\n                f\"Path to masks directory {masks_dir} does not exist.\"\n            )\n        masks_dir = os.path.abspath(masks_dir)\n        load_masks = True\n\n    check_valid_pipeline(cfg)\n\n    # main_workflow\n    main_workflow = pe.Workflow(name=pipeline_name)\n    main_workflow.base_dir = nipype_dir\n    fet_pipe = create_full_pipeline(cfg, load_masks)\n\n    output_query = {\n        \"stacks\": {\n            \"datatype\": \"anat\",\n            \"suffix\": \"T2w\",\n            \"extension\": [\"nii\", \".nii.gz\"],\n        },\n    }\n    if load_masks:\n        output_query[\"masks\"] = {\n            \"datatype\": \"anat\",\n            \"suffix\": \"mask\",\n            \"extension\": [\"nii\", \".nii.gz\"],\n        }\n\n    # datasource\n    datasource = create_datasource(\n        output_query,\n        data_dir,\n        nipype_dir,\n        subjects,\n        sessions,\n        acquisitions,\n        extra_derivatives=masks_dir,\n    )\n\n    input_data = pe.Workflow(name=\"input\")\n\n    output_fields = [\"stacks\"]\n    if load_masks:\n        output_fields.append(\"masks\")\n    output = pe.Node(\n        niu.IdentityInterface(fields=output_fields), name=\"outputnode\"\n    )\n    input_data.connect(datasource, \"stacks\", output, \"stacks\")\n    if load_masks:\n        input_data.connect(datasource, \"masks\", output, \"masks\")\n\n    main_workflow.connect(\n        input_data, \"outputnode.stacks\", fet_pipe, \"inputnode.stacks\"\n    )\n    if load_masks:\n        main_workflow.connect(\n            datasource, \"outputnode.masks\", fet_pipe, \"inputnode.masks\"\n        )\n\n    # Reconstruction data sink:\n    pipeline_name = get_pipeline_name(cfg)\n\n    # Preprocessing data sink:\n    if save_intermediates:\n        datasink_path_intermediate = os.path.join(out_dir, \"preprocessing\")\n        os.makedirs(datasink_path_intermediate, exist_ok=True)\n        create_description_file(\n            datasink_path_intermediate, \"preprocessing\", cfg=cfg.reconstruction\n        )\n\n        # Create a datasink for the preprocessing pipeline\n        preprocessing_datasink_denoised = create_bids_datasink(\n            out_dir=datasink_path_intermediate,\n            pipeline_name=\"preprocessing\",  # Use combined name\n            strip_dir=main_workflow.base_dir,\n            name=\"preprocessing_datasink_denoised\",\n            desc_label=\"denoised\",\n        )\n        preprocessing_datasink_masked = create_bids_datasink(\n            out_dir=datasink_path_intermediate,\n            pipeline_name=\"preprocessing\",  # Use combined name\n            strip_dir=main_workflow.base_dir,\n            name=\"preprocessing_datasink_cropped\",\n            desc_label=\"cropped\",\n        )\n\n        # Connect the pipeline to the datasinks\n        main_workflow.connect(\n            fet_pipe,\n            \"Preprocessing.outputnode.stacks\",\n            preprocessing_datasink_denoised,\n            \"@stacks\",\n        )\n        main_workflow.connect(\n            fet_pipe,\n            \"Preprocessing.outputnode.masks\",\n            preprocessing_datasink_masked,\n            \"@masks\",\n        )\n\n    recon_datasink = create_bids_datasink(\n        out_dir=out_dir,\n        pipeline_name=pipeline_name,\n        strip_dir=main_workflow.base_dir,\n        name=\"final_recon_datasink\",\n        rec_label=cfg.reconstruction.pipeline,\n    )\n\n    # Create another datasink for the segmentation pipeline\n    seg_datasink = create_bids_datasink(\n        out_dir=out_dir,\n        pipeline_name=pipeline_name,\n        strip_dir=main_workflow.base_dir,\n        name=\"final_seg_datasink\",\n        rec_label=cfg.reconstruction.pipeline,\n        seg_label=cfg.segmentation.pipeline,\n    )\n\n    surf_datasink = create_bids_datasink(\n        out_dir=out_dir,\n        pipeline_name=pipeline_name,\n        strip_dir=main_workflow.base_dir,\n        name=\"final_surf_datasink\",\n        rec_label=cfg.reconstruction.pipeline,\n        seg_label=cfg.segmentation.pipeline,\n        surf_label=cfg.surface.pipeline,\n    )\n\n    # Connect the pipeline to the datasink\n    main_workflow.connect(\n        fet_pipe, \"outputnode.output_srr\", recon_datasink, f\"@{pipeline_name}\"\n    )\n    main_workflow.connect(\n        fet_pipe,\n        \"outputnode.output_seg\",\n        seg_datasink,\n        f\"@{cfg.segmentation.pipeline}\",\n    )\n\n    main_workflow.connect(\n        fet_pipe,\n        \"outputnode.output_surf_lh\",\n        surf_datasink,\n        \"@surf_lh\",\n    )\n\n    main_workflow.connect(\n        fet_pipe,\n        \"outputnode.output_surf_rh\",\n        surf_datasink,\n        \"@surf_rh\",\n    )\n\n    if cfg.save_graph:\n        main_workflow.write_graph(\n            graph2use=\"colored\",\n            format=\"png\",\n            simple_form=True,\n        )\n\n    main_workflow.run(\n        plugin=\"MultiProc\",\n        plugin_args={\"n_procs\": nprocs, \"status_callback\": status_line},\n    )\n</code></pre>"},{"location":"api_workflows/#fetpype.workflows.pipeline_rec","title":"<code>pipeline_rec</code>","text":""},{"location":"api_workflows/#fetpype.workflows.pipeline_rec.create_rec_workflow","title":"<code>create_rec_workflow(data_dir, masks_dir, out_dir, nipype_dir, subjects, sessions, acquisitions, cfg_path, nprocs, debug=False, verbose=False)</code>","text":"<p>Instantiates and runs the entire workflow of the fetpype pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the BIDS directory that contains anatomical images.</p> required <code>masks_dir</code> <code>str</code> <p>Path to the BIDS directory that contains brain masks.</p> required <code>out_dir</code> <code>str</code> <p>Path to the output directory (will be created if not already existing). Previous outputs may be overriden.</p> required <code>nipype_dir</code> <code>str</code> <p>Path to the nipype directory.</p> required <code>subjects</code> <code>list[str]</code> <p>List of subject IDs matching the BIDS specification (e.g., sub-[SUB1], sub-[SUB2], ...).</p> required <code>sessions</code> <code>list[str]</code> <p>List of session IDs matching the BIDS specification (e.g., ses-[SES1], ses-[SES2], ...).</p> required <code>acquisitions</code> <code>list[str]</code> <p>List of acquisition names matching the BIDS specification (e.g., acq-[ACQ1], ...).</p> required <code>cfg_path</code> <code>str</code> <p>Path to a hydra  configuration file (YAML) specifying pipeline parameters.</p> required <code>nprocs</code> <code>int</code> <p>Number of processes to be launched by MultiProc.</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug mode.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose mode.</p> <code>False</code> Source code in <code>fetpype/workflows/pipeline_rec.py</code> <pre><code>def create_rec_workflow(\n    data_dir,\n    masks_dir,\n    out_dir,\n    nipype_dir,\n    subjects,\n    sessions,\n    acquisitions,\n    cfg_path,\n    nprocs,\n    debug=False,\n    verbose=False,\n):\n    \"\"\"\n    Instantiates and runs the entire workflow of the fetpype pipeline.\n\n    Args:\n        data_dir (str):\n            Path to the BIDS directory that contains anatomical images.\n        masks_dir (str):\n            Path to the BIDS directory that contains brain masks.\n        out_dir (str):\n            Path to the output directory (will be created if not already\n            existing). Previous outputs may be overriden.\n        nipype_dir (str):\n            Path to the nipype directory.\n        subjects (list[str], optional):\n            List of subject IDs matching the BIDS specification\n            (e.g., sub-[SUB1], sub-[SUB2], ...).\n        sessions (list[str], optional):\n            List of session IDs matching the BIDS specification\n            (e.g., ses-[SES1], ses-[SES2], ...).\n        acquisitions (list[str], optional):\n            List of acquisition names matching the BIDS specification\n            (e.g., acq-[ACQ1], ...).\n        cfg_path (str):\n            Path to a hydra  configuration file (YAML) specifying pipeline\n            parameters.\n        nprocs (int):\n            Number of processes to be launched by MultiProc.\n        debug (bool):\n            Whether to enable debug mode.\n        verbose (bool):\n            Whether to enable verbose mode.\n    \"\"\"\n\n    cfg = init_and_load_cfg(cfg_path)\n\n    pipeline_name = get_pipeline_name(cfg, only_rec=True)\n    data_dir, out_dir, nipype_dir = check_and_update_paths(\n        data_dir, out_dir, nipype_dir, pipeline_name\n    )\n\n    setup_logging(\n        base_dir=nipype_dir,\n        debug=debug,\n        verbose=verbose,\n        capture_prints=True,\n    )\n\n    load_masks = False\n    if masks_dir is not None:\n        # Check it exists\n        if not os.path.exists(masks_dir):\n            raise ValueError(\n                f\"Path to masks directory {masks_dir} does not exist.\"\n            )\n        masks_dir = os.path.abspath(masks_dir)\n        load_masks = True\n    check_valid_pipeline(cfg)\n    # if general, pipeline is not in params ,create it and set it to niftymic\n\n    # main_workflow\n    main_workflow = pe.Workflow(name=pipeline_name)\n    main_workflow.base_dir = nipype_dir\n    fet_pipe = create_rec_pipeline(cfg, load_masks)\n\n    output_query = {\n        \"stacks\": {\n            \"datatype\": \"anat\",\n            \"suffix\": \"T2w\",\n            \"extension\": [\"nii\", \".nii.gz\"],\n        },\n    }\n    if load_masks:\n        output_query[\"masks\"] = {\n            \"datatype\": \"anat\",\n            \"suffix\": \"mask\",\n            \"extension\": [\"nii\", \".nii.gz\"],\n        }\n\n    # datasource\n    datasource = create_datasource(\n        output_query,\n        data_dir,\n        nipype_dir,\n        subjects,\n        sessions,\n        acquisitions,\n        extra_derivatives=masks_dir,\n    )\n    main_workflow.connect(datasource, \"stacks\", fet_pipe, \"inputnode.stacks\")\n    if load_masks:\n        main_workflow.connect(datasource, \"masks\", fet_pipe, \"inputnode.masks\")\n\n    # DataSink\n\n    # Reconstruction data sink:\n    pipeline_name = cfg.reconstruction.pipeline\n    create_description_file(out_dir, pipeline_name, cfg=cfg.reconstruction)\n\n    datasink = create_bids_datasink(\n        out_dir=out_dir,\n        pipeline_name=pipeline_name,\n        strip_dir=main_workflow.base_dir,\n        name=\"final_recon_datasink\",\n        rec_label=cfg.reconstruction.pipeline,\n    )\n    # datasink.inputs.base_directory = datasink_path\n\n    # Connect the pipeline to the datasink\n    main_workflow.connect(\n        fet_pipe, \"outputnode.output_srr\", datasink, f\"@{pipeline_name}\"\n    )\n\n    if cfg.save_graph:\n        main_workflow.write_graph(\n            graph2use=\"colored\",\n            format=\"png\",\n            simple_form=True,\n        )\n\n    main_workflow.run(\n        plugin=\"MultiProc\",\n        plugin_args={\"n_procs\": nprocs, \"status_callback\": status_line},\n    )\n</code></pre>"},{"location":"api_workflows/#fetpype.workflows.pipeline_seg","title":"<code>pipeline_seg</code>","text":""},{"location":"api_workflows/#fetpype.workflows.pipeline_seg.create_seg_workflow","title":"<code>create_seg_workflow(data_dir, out_dir, nipype_dir, subjects, sessions, acquisitions, cfg_path, nprocs, ignore_checks=False, debug=False, verbose=False)</code>","text":"<p>Instantiates and runs the entire workflow of the fetpype pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the BIDS directory that contains anatomical images.</p> required <code>out_dir</code> <code>str</code> <p>Path to the output directory (will be created if not already existing). Previous outputs may be overriden.</p> required <code>nipype_dir</code> <code>str</code> <p>Path to the nipype directory.</p> required <code>subjects</code> <code>list[str]</code> <p>List of subject IDs matching the BIDS specification (e.g., sub-[SUB1], sub-[SUB2], ...).</p> required <code>sessions</code> <code>list[str]</code> <p>List of session IDs matching the BIDS specification (e.g., ses-[SES1], ses-[SES2], ...).</p> required <code>acquisitions</code> <code>list[str]</code> <p>List of acquisition names matching the BIDS specification (e.g., acq-[ACQ1], ...).</p> required <code>cfg_path</code> <code>str</code> <p>Path to a hydra  configuration file (YAML) specifying pipeline parameters.</p> required <code>nprocs</code> <code>int</code> <p>Number of processes to be launched by MultiProc.</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug mode.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose mode.</p> <code>False</code> Source code in <code>fetpype/workflows/pipeline_seg.py</code> <pre><code>def create_seg_workflow(\n    data_dir,\n    out_dir,\n    nipype_dir,\n    subjects,\n    sessions,\n    acquisitions,\n    cfg_path,\n    nprocs,\n    ignore_checks=False,\n    debug=False,\n    verbose=False,\n):\n    \"\"\"\n    Instantiates and runs the entire workflow of the fetpype pipeline.\n\n    Args:\n        data_dir (str):\n            Path to the BIDS directory that contains anatomical images.\n        out_dir (str):\n            Path to the output directory (will be created if not already\n            existing). Previous outputs may be overriden.\n        nipype_dir (str):\n            Path to the nipype directory.\n        subjects (list[str], optional):\n            List of subject IDs matching the BIDS specification\n            (e.g., sub-[SUB1], sub-[SUB2], ...).\n        sessions (list[str], optional):\n            List of session IDs matching the BIDS specification\n            (e.g., ses-[SES1], ses-[SES2], ...).\n        acquisitions (list[str], optional):\n            List of acquisition names matching the BIDS specification\n            (e.g., acq-[ACQ1], ...).\n        cfg_path (str):\n            Path to a hydra  configuration file (YAML) specifying pipeline\n            parameters.\n        nprocs (int):\n            Number of processes to be launched by MultiProc.\n        debug (bool):\n            Whether to enable debug mode.\n        verbose (bool):\n            Whether to enable verbose mode.\n    \"\"\"\n    cfg = init_and_load_cfg(cfg_path)\n    pipeline_name = get_pipeline_name(cfg, only_seg=True)\n    data_dir, out_dir, nipype_dir = check_and_update_paths(\n        data_dir, out_dir, nipype_dir, pipeline_name\n    )\n\n    setup_logging(\n        base_dir=nipype_dir,\n        debug=debug,\n        verbose=verbose,\n        capture_prints=True,\n    )\n\n    check_valid_pipeline(cfg)\n    # if general, pipeline is not in params ,create it and set it to niftymic\n\n    data_desc = os.path.join(data_dir, \"dataset_description.json\")\n    if not ignore_checks:\n        if os.path.exists(data_desc):\n            with open(data_desc, \"r\") as f:\n                data_desc = json.load(f)\n            name = data_desc.get(\"Name\", None)\n            if \"_\" in name:\n                name = name.split(\"_\")[0]\n            if name not in VALID_RECONSTRUCTION:\n                raise ValueError(\n                    f\"Method name &lt;{data_desc['Name']}&gt; is not a valid \"\n                    \"reconstruction method. Are you sure that you are \"\n                    \"passing a reconstructed dataset?\\n\"\n                    \"If you know what you are doing, you can ignore \"\n                    \"this error by adding --ignore_check to the command line.\"\n                )\n        else:\n            raise ValueError(\n                f\"dataset_description.json file not found in {data_dir}. \"\n                \"Please provide a valid BIDS directory.\"\n            )\n    # main_workflow\n    main_workflow = pe.Workflow(name=pipeline_name)\n    main_workflow.base_dir = nipype_dir\n    fet_pipe = create_seg_pipeline(cfg)\n\n    output_query = {\n        \"srr_volume\": {\n            \"datatype\": \"anat\",\n            \"suffix\": \"T2w\",\n            \"extension\": [\"nii\", \".nii.gz\"],\n        }\n    }\n\n    # datasource\n    datasource = create_datasource(\n        output_query,\n        data_dir,\n        nipype_dir,\n        subjects,\n        sessions,\n        acquisitions,\n    )\n\n    # in both cases we connect datsource outputs to main pipeline\n    main_workflow.connect(\n        datasource, \"srr_volume\", fet_pipe, \"inputnode.srr_volume\"\n    )\n\n    # DataSink\n\n    # Segmentation data sink:\n    pipeline_name = cfg.segmentation.pipeline\n    datasink_path = os.path.join(out_dir, pipeline_name)\n    # Create json file to make it BIDS compliant if doesnt exist\n    # Eventually, add all parameters to the json file\n    os.makedirs(datasink_path, exist_ok=True)\n\n    # Create datasink\n    pipeline_name = cfg.segmentation.pipeline\n    os.makedirs(datasink_path, exist_ok=True)\n    prev_desc = os.path.join(data_dir, \"dataset_description.json\")\n    if not os.path.exists(prev_desc):\n        prev_desc = None\n\n    create_description_file(\n        out_dir, pipeline_name, prev_desc, cfg.segmentation\n    )\n    # Create another datasink for the segmentation pipeline\n    seg_datasink = create_bids_datasink(\n        out_dir=out_dir,\n        pipeline_name=pipeline_name,\n        strip_dir=main_workflow.base_dir,\n        name=\"final_seg_datasink\",\n        rec_label=cfg.reconstruction.pipeline,\n        seg_label=cfg.segmentation.pipeline,\n    )\n    # Add the base directory\n\n    # Connect the pipeline to the datasink\n    main_workflow.connect(\n        fet_pipe, \"outputnode.output_seg\", seg_datasink, pipeline_name\n    )\n\n    if cfg.save_graph:\n        main_workflow.write_graph(\n            graph2use=\"colored\",\n            format=\"png\",\n            simple_form=True,\n        )\n    main_workflow.run(\n        plugin=\"MultiProc\",\n        plugin_args={\"n_procs\": nprocs, \"status_callback\": status_line},\n    )\n</code></pre>"},{"location":"configs/","title":"Config files","text":"<p>Config files are the bread and butter of fetpype. They allow to define how the data should be processed within a container and what output should be returned. They also specify optional parameters that can be modified by the user, without touching at the code. They are based on the Hydra framework, that allows to flexibly parse a hierarchical structure of <code>.yaml</code> configs into a unified data structure. </p>"},{"location":"configs/#the-general-structure","title":"The general structure","text":"<p>Fetpype acts as a wrapper around calls to various containers, and uses a limited set of tags in each dedicated node to construct the command that will be called. Config files define the commands that will be called by fetpype. Fetpype starts from a master config located at <code>configs/default_docker.yaml</code> (or <code>default_sg.yaml</code> for singularity), with the following structure:</p> <p><pre><code>defaults:\n  - preprocessing/default # Default preprocessing\n  - reconstruction/nesvor # NeSVoR reconstruction -- You can choose between svrtk, nifymic or nesvor\n  - segmentation/bounti   # BOUNTI segmentation     \n  - _self_\ncontainer: \"docker\"       # Running on docker (other option is singularity)\nreconstruction:           # Generic reconstruction arguments\n  output_resolution: 0.8  # Target resolution for reconstruction\nsave_graph: True\n</code></pre> Each of the <code>defaults</code> entries call to other config files, located respectively at <code>configs/preprocessing/default.yaml</code>, <code>configs/reconstruction/nesvor.yaml</code>, etc.</p>"},{"location":"configs/#example-of-a-specific-config","title":"Example of a specific config","text":"<p>Let's look at an example of how a specific config is structured. If we open <code>configs/reconstruction/nesvor.yaml</code>, we see </p> <pre><code>pipeline: \"nesvor\"\ndocker: \n  cmd: \"docker run --gpus '\\\"device=0\\\"' &lt;mount&gt; junshenxu/nesvor:v0.5.0 \n    nesvor reconstruct \n    --input-stacks &lt;input_stacks&gt; \n    --stack-masks &lt;input_masks&gt; \n    --output-volume &lt;output_volume&gt; \n    --batch-size 4096 \n    --n-levels-bias 1\"\nsingularity:\n  cmd: \"singularity exec --bind &lt;singularity_mount&gt; --nv &lt;singularity_path&gt;/nesvor.sif \n    nesvor reconstruct \n    --input-stacks &lt;input_stacks&gt; \n    --stack-masks &lt;input_masks&gt; \n    --output-volume &lt;output_volume&gt; \n    --batch-size 4096 \n    --n-levels-bias 1\"\nargs:\n    path_to_output: \"nesvor.nii.gz\"\n</code></pre> <p>In this config, we see a common structure that we will find in most of the configs. There is a <code>docker</code> and a <code>singularity</code> entry that define the command (<code>cmd</code>) that fetpype will run. The command has specific tags (marked as <code>&lt;tag&gt;</code>) that can be specified. The structure is globally similar for all configs, but specific information on how config files are structured is provided in the pipelines page.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Fetpype is an open source project and welcomes contributions! Here are some ideas on how to help:</p> <ul> <li>Writing and improving the documentation</li> <li>Reporting or fixing bugs</li> <li>Adding your amazing pre-processing/reconstruction/segmentation/surface extraction method to fetpype.</li> <li>Other features or enhancements</li> </ul>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Bugs and issues can be reported on GitHub. Please check the bug has not already been reported by searching the issue tracker before submitting a new issue!</p>"},{"location":"contributing/#adding-your-method-to-fetpype","title":"Adding your method to fetpype","text":"<p>Fetpype is designed to be a modular wrapper around exiting tools. It is based running calls to docker/singularity containers defined in a yaml file.</p> <p>If you wish to incorporate your method into fetpype, please have a look at the page describing the pipeline step to which you want to contribute (i.e. preprocessing, reconstruction, segmentation or surface extraction). Each page describes what the step does, what kind of input and output are expected, and the tags that can be provided to interact with your container image. If you have trouble in making your method compatible with fetpype, don't hesitate opening an issue on GitHub.</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Feature requests can be made on GitHub as well!</p>"},{"location":"contributing/#solving-issues","title":"Solving Issues","text":"<p>Any new feature, bug fix or documentation contribution is welcome as a pull request (PR)! To do that, simply open a new GitHub PR with your contribution. Please include a clear description of the problem, refer to any relevant issues, and explain how your contribution solves the problem, and on which data you were able to test it.</p>"},{"location":"input_data/","title":"BIDS Input Data Preparation","text":"<p>This guide explains how to prepare your fetal brain MRI data in BIDS (Brain Imaging Data Structure) format for use with Fetpype. </p>"},{"location":"input_data/#overview","title":"Overview","text":"<p>Fetpype expects input data to follow the BIDS specification for neuroimaging datasets. This standardized format ensures reproducibility, facilitates data sharing, and enables automatic processing pipeline execution.</p>"},{"location":"input_data/#bids-structure","title":"BIDS Structure","text":"<p>The BIDS structure is as follows:</p> <pre><code>dataset/\n\u251c\u2500\u2500 dataset_description.json           # Dataset metadata (required)\n\u251c\u2500\u2500 participants.tsv                   # Subject information (required for dHCP)\n\u251c\u2500\u2500 README                             # Dataset description (recommended)\n\u251c\u2500\u2500 CHANGES                            # Version history (recommended)\n\u2514\u2500\u2500 sub-&lt;subject_id&gt;/                  # Subject directories\n    \u2514\u2500\u2500 [ses-&lt;session_id&gt;/]            # Session directories (optional)\n        \u2514\u2500\u2500 anat/                      # Anatomical data directory\n            \u251c\u2500\u2500 sub-&lt;subject_id&gt;[_ses-&lt;session_id&gt;][_acq-&lt;acquisition&gt;]_run-&lt;run_number&gt;_T2w.nii.gz\n            \u251c\u2500\u2500 sub-&lt;subject_id&gt;[_ses-&lt;session_id&gt;][_acq-&lt;acquisition&gt;]_run-&lt;run_number&gt;_T2w.nii.gz\n            \u251c\u2500\u2500 sub-&lt;subject_id&gt;[_ses-&lt;session_id&gt;][_acq-&lt;acquisition&gt;]_run-&lt;run_number&gt;_T2w.nii.gz\n            \u2514\u2500\u2500 ...\n</code></pre> <p>Note</p> <p>More information on the structure of the <code>participants.tsv</code> file needed for the dHCP pipeline is provided on this page.</p>"},{"location":"input_data/#naming-conventions","title":"Naming Conventions","text":"<p>The naming convention for the files is as follows:</p> <pre><code>sub-&lt;subject_id&gt;[_ses-&lt;session_id&gt;][_acq-&lt;acquisition&gt;]_run-&lt;run_number&gt;_T2w.nii.gz\n</code></pre> <p>The following entities are supported:</p> <ul> <li><code>sub-&lt;subject_id&gt;</code>: Subject identifier</li> <li><code>ses-&lt;session_id&gt;</code>: Session identifier</li> <li><code>acq-&lt;acquisition&gt;</code>: Acquisition identifier</li> <li><code>run-&lt;run_number&gt;</code>: Run number</li> </ul>"},{"location":"input_data/#dicom-to-bids-conversion","title":"DICOM to BIDS Conversion","text":"<p>There are several tools that can be used to convert DICOM to BIDS. We recommend using dcm2bids. Other options are to use dcm2niix to create the nifti files and then convert the files to BIDS</p>"},{"location":"output_data/","title":"Output Data Structure","text":"<p>Fetpype organizes all processed data following the BIDS (Brain Imaging Data Structure) derivatives convention. This ensures your results are shareable, reproducible, and compatible with other neuroimaging tools.</p> <p>For transforming raw data into the BIDS structure Fetpype expects, see: Input Data Structure.</p>"},{"location":"output_data/#the-datasink-module","title":"The DataSink Module","text":"<p>Fetpype uses a custom DataSink (see <code>create_bids_datasink</code> in <code>fetpype/utils/utils_bids.py</code>) to organize and rename pipeline outputs into BIDS-compliant structures. It wraps Nipype's DataSink and applies regex substitutions to ensure outputs are properly named and structured for downstream analysis. For a more detailed explanation of the DataSink module, see the Nipype documentation or the nipype-tutorial on DataSink.</p>"},{"location":"output_data/#how-datasink-works","title":"How DataSink Works","text":"<ul> <li>Input: Nipype working directory outputs (often with non-BIDS names)</li> <li>Processing: Applies a set of regex and simple substitutions to transform paths and filenames into BIDS-compliant outputs</li> <li>Output: Files organized in the BIDS derivatives structure, with correct naming and metadata</li> </ul>"},{"location":"output_data/#example-output-structure","title":"Example Output Structure","text":"<pre><code>derivatives/\n\u251c\u2500\u2500 &lt;pipeline_name&gt;/\n\u2502   \u251c\u2500\u2500 dataset_description.json\n\u2502   \u2514\u2500\u2500 sub-&lt;subject&gt;/\n\u2502       \u2514\u2500\u2500 [ses-&lt;session&gt;/]\n\u2502           \u2514\u2500\u2500 anat/\n\u2502               \u2514\u2500\u2500 sub-&lt;subject&gt;_[ses-&lt;session&gt;]_[rec-&lt;reconstruction&gt;]_[seg-&lt;segmentation&gt;]_[surf-surface]_[desc-&lt;description&gt;]_&lt;suffix&gt;.nii.gz\n</code></pre>"},{"location":"output_data/#file-naming-convention","title":"File Naming Convention","text":"Input (Nipype working dir) Output (BIDS derivatives) <code>preprocessing_wf/_session_01_subject_sub-01/denoise_wf/_denoising/sub-01_ses-01_run-1_T2w_noise_corrected.nii.gz</code> <code>sub-01/ses-01/anat/sub-01_ses-01_run-1_desc-denoised_T2w.nii.gz</code> <code>nesvor_pipeline_wf/_session_02_subject_sub-01/recon_node/recon.nii.gz</code> <code>sub-01/ses-02/anat/sub-01_ses-02_rec-nesvor_T2w.nii.gz</code> <code>segmentation_wf/_session_01_subject_sub-01/seg_node/input_srr-mask-brain_bounti-19.nii.gz</code> <code>sub-01/ses-01/anat/sub-01_ses-01_rec-nesvor_seg-bounti_dseg.nii.gz</code> <code>surface_wf/_session_01_subject_sub-01/seg_node/surf.gii</code> <code>sub-01/ses-01/anat/sub-01_ses-01_rec-nesvor_seg-bounti_surf-surface-surf.gii</code>"},{"location":"output_data/#core-bids-entities","title":"Core BIDS Entities","text":"<ul> <li><code>sub-XX</code>: Subject identifier</li> <li><code>ses-XX</code>: Session identifier (omitted if no sessions)</li> <li><code>run-X</code>: Run number (from original stacks, preserved in preprocessing)</li> <li><code>rec-METHOD</code>: Reconstruction method (e.g., rec-nesvor)</li> <li><code>seg-METHOD</code>: Segmentation method (e.g., seg-bounti)</li> <li><code>surf-METHOD</code>: Surface extraction method (e.g., surf-surfpype)</li> <li><code>desc-DESCRIPTION</code>: Processing description (e.g., desc-denoised)</li> <li>Suffixes: <code>T2w</code>, <code>dseg</code>, <code>mask</code>, <code>surf.gii</code>, <code>shape.gii</code></li> </ul>"},{"location":"output_data/#datasink-regex-and-substitution-rules","title":"DataSink Regex and Substitution Rules","text":"<p>The DataSink applies context-specific regex rules to convert Nipype outputs to BIDS-compliant names. Some examples of rules that are in use for various pipelines:</p> <ul> <li>Preprocessing (denoised):</li> <li>Input: .../denoise_wf/.../sub-01_ses-01_run-1_T2w_noise_corrected.nii.gz</li> <li>Output: sub-01/ses-01/anat/sub-01_ses-01_run-1_desc-denoised_T2w.nii.gz</li> <li>Preprocessing (cropped):</li> <li>Input: .../crop_wf/.../sub-01_ses-01_run-1_mask.nii</li> <li>Output: sub-01/ses-01/anat/sub-01_ses-01_run-1_desc-cropped_mask.nii</li> <li>Reconstruction:</li> <li>Input: .../recon_node/recon.nii.gz</li> <li>Output: sub-01/ses-02/anat/sub-01_ses-02_rec-nesvor_T2w.nii.gz</li> <li>Segmentation:</li> <li>Input: .../seg_node/input_srr-mask-brain_bounti-19.nii.gz</li> <li>Output: sub-01/ses-01/anat/sub-01_ses-01_rec-nesvor_seg-bounti_dseg.nii.gz</li> <li>Surface extraction:</li> <li>Input: .../surf_node/surf.gii</li> <li>Output: sub-01/ses-01/anat/sub-01_ses-01_rec-nesvor_seg-bounti_surf-surface_surf.gii</li> </ul> <p>Cleanup rules (applied to all outputs): - Remove doubled prefixes (e.g., <code>sub-sub-</code> \u2192 <code>sub-</code>) - Collapse multiple underscores or slashes - Remove <code>None</code> session labels - Fix repeated extensions (e.g., <code>.nii.gz.gz</code> \u2192 <code>.nii.gz</code>)</p>"},{"location":"output_data/#customization","title":"Customization","text":"<p>You can add custom substitutions for specialized file patterns using the <code>custom_subs</code> and <code>custom_regex_subs</code> arguments to <code>create_bids_datasink</code>.</p> <p>Example: <pre><code>custom_regex_subs = [\n    (r\"_custom_suffix\", \"\"),\n    (r\"temp_\", \"\")\n]\ndatasink = create_bids_datasink(\n    ..., custom_regex_subs=custom_regex_subs\n)\n</code></pre></p>"},{"location":"output_data/#example-integrating-datasink-in-a-pipeline","title":"Example: Integrating DataSink in a Pipeline","text":"<pre><code>from fetpype.utils.utils_bids import create_bids_datasink\n\ndatasink = create_bids_datasink(\n    out_dir=\"/path/to/derivatives\",\n    pipeline_name=\"nesvor_bounti\",\n    strip_dir=\"/path/to/nipype/workdir\",\n    rec_label=\"nesvor\",\n    seg_label=\"bounti\"\n)\n\nworkflow.connect(\n    node, \"output\", datasink, \"@nesvor_bounti\"\n)\n</code></pre>"},{"location":"output_data/#best-practices","title":"Best Practices","text":"<ul> <li>Always set <code>strip_dir</code>: This should point to the Nipype base working directory. If not set, DataSink will raise an error.</li> <li>Use descriptive labels: For <code>rec_label</code>, <code>seg_label</code>, <code>surf_label</code>, and <code>desc_label</code>, use values that reflect the actual processing method (e.g., <code>nesvor</code>, <code>bounti</code>, <code>surface</code>, <code>denoised</code>).</li> <li>Pipeline names: Should match the processing chain (e.g., <code>nesvor_bounti_surface</code>, <code>preprocessing</code>).</li> <li>Test your DataSink configuration: Use unit tests or inspect outputs to ensure correct organization and naming.</li> </ul>"},{"location":"output_data/#troubleshooting","title":"Troubleshooting","text":"<p>If your pipeline finishes but you don't see outputs in <code>derivatives/</code>, check: - The Nipype working directory for your results (they may not have been moved/renamed correctly). - That your input BIDS dataset uses standard subject/session naming. - That there are no special characters or unexpected extensions in your filenames. - That session information is present/absent as expected.</p> <p>If issues persist, please open an issue on GitHub with: - Your input BIDS directory structure. - The command and config file used. - Example file paths from your Nipype working directory.</p>"},{"location":"pipelines/","title":"Available methods","text":"<p>The currently implemented steps of the pipelines and their configuration files are defined in the following pages: - Preprocessing - Reconstruction - Reconstruction - Surface extraction</p>"},{"location":"preprocessing/","title":"Preprocessing","text":"<p>Pre-processing starts from multiple T2-weighted stacks and processed each stack individually to prepare them for super-resolution reconstruction.</p>"},{"location":"preprocessing/#available-tools","title":"Available tools","text":"<p>Fetpype pre-processes the data in the following order. </p> <p>Data Loading (\u2192 Brain extraction) \u2192 Resolution checks \u2192 Cropping \u2192 Denoising  \u2192 Bias field correction \u2192 Output checks</p> <p>The three steps in boldface are run from the fetpype_utils container. The code for building the docker is available at this repository.</p> <p>Currently, we implement the following options:</p> <ul> <li>Brain extraction: <code>Fetal-BET</code><sup>1</sup> (default) and <code>MONAIfbs</code><sup>2</sup></li> <li>Denoising: Non-local means denosing<sup>3</sup> (calling ANTS' <code>DenoiseImage</code>)</li> <li>Bias field correction: N4 bias field correction<sup>4</sup></li> </ul>"},{"location":"preprocessing/#config-structure","title":"Config structure","text":"<p>The config file is structured as follows: <pre><code>brain_extraction:\n  docker:\n    cmd: \"docker run --gpus all &lt;mount&gt; thsanchez/fetpype_utils:latest run_brain_extraction \n      --input_stacks &lt;input_stacks&gt; \n      --output_masks &lt;output_masks&gt; \n      --method fet_bet\"\n  singularity:\n    cmd: \"singularity run --bind &lt;singularity_mount&gt; --nv\n      &lt;singularity_path&gt;/fetpype_utils.sif\n      run_brain_extraction\n      --input_stacks &lt;input_stacks&gt; \n      --output_masks &lt;output_masks&gt; --method monaifbs\"\n\ncheck_stacks_and_masks:\n  enabled: true\n\ndenoising:\n  enabled: true\n  docker:\n    cmd: \"docker run &lt;mount&gt; thsanchez/fetpype_utils:latest run_denoising \n      --input_stacks &lt;input_stacks&gt; \n      --output_stacks &lt;output_stacks&gt;\"\n  singularity:\n    cmd: \"singularity run --bind &lt;singularity_mount&gt;\n      &lt;singularity_path&gt;/fetpype_utils.sif\n      run_denoising\n      --input_stacks &lt;input_stacks&gt; \n      --output_stacks &lt;output_stacks&gt;\"\ncropping:\n  enabled: true\n\nbias_correction:\n  enabled: true\n  docker:\n    cmd: \"docker run &lt;mount&gt; thsanchez/fetpype_utils:latest run_bias_field_correction \n      --input_stacks &lt;input_stacks&gt; \n      --input_masks &lt;input_masks&gt; \n      --output_stacks &lt;output_stacks&gt;\"\n  singularity:\n    cmd: \"singularity run --bind &lt;singularity_mount&gt;\n      &lt;singularity_path&gt;/fetpype_utils.sif\n      run_bias_field_correction\n      --input_stacks &lt;input_stacks&gt; \n      --input_masks &lt;input_masks&gt; \n      --output_stacks &lt;output_stacks&gt;\"\n</code></pre></p> <p>Note</p> <p>All the container runs use the command above and are passed through the function <code>run_prepro_cmd</code></p> <p>Note</p> <ul> <li>Each pre-processing step that can be disabled has a boolean entry <code>enabled: true</code> that can be set to false.</li> <li>The steps that rely on a container are set with a list of valid tags</li> </ul>"},{"location":"preprocessing/#tags","title":"Tags","text":"<p>There are a limited set of tags that can be used for preprocessing: </p> Command Description Comments <code>&lt;mount&gt;</code> Where the different folders will be mounted on Docker Docker only <code>&lt;singularity_mount&gt;</code> Where the different folders will be mounted on Singularity Singularity only <code>&lt;singularity_path&gt;</code> The base path of the Singularity image Singularity only <code>&lt;input_stacks&gt;</code> The list of inputs stacks will be given as arguments Mutually exclusive with <code>&lt;input_dir&gt;</code> <code>&lt;input_masks&gt;</code> The list of inputs masks will be given as arguments Mutually exclusive with <code>&lt;input_masks_dir&gt;</code> <code>&lt;output_stacks&gt;</code> The list of output stacks <code>&lt;output_masks&gt;</code> The list of output masks <ol> <li> <p>Razieh Faghihpirayesh, Davood Karimi, Deniz Erdo\u011fmu\u015f, and Ali Gholipour. Fetal-BET: brain extraction tool for fetal MRI. IEEE Open Journal of Engineering in Medicine and Biology, 2024.\u00a0\u21a9</p> </li> <li> <p>Marta Ranzini, Lucas Fidon, S\u00e9bastien Ourselin, Marc Modat, and Tom Vercauteren. MONAIfbs: monai-based fetal brain mri deep learning segmentation. arXiv preprint arXiv:2103.13314, 2021.\u00a0\u21a9</p> </li> <li> <p>Jos\u00e9 V Manj\u00f3n, Pierrick Coup\u00e9, Luis Mart\u00ed-Bonmat\u00ed, D Louis Collins, and Montserrat Robles. Adaptive non-local means denoising of mr images with spatially varying noise levels. Journal of Magnetic Resonance Imaging, 31(1):192\u2013203, 2010.\u00a0\u21a9</p> </li> <li> <p>Nicholas J Tustison, Brian B Avants, Philip A Cook, Yuanjie Zheng, Alexander Egan, Paul A Yushkevich, and James C Gee. N4ITK: improved n3 bias correction. IEEE transactions on medical imaging, 29(6):1310\u20131320, 2010.\u00a0\u21a9</p> </li> </ol>"},{"location":"reconstruction/","title":"Reconstruction","text":"<p>In the super-resolution reconstruction steps, an algorithm will take as input several pre-processed stacks and output a single super-resolution reconstructed volume. This volume can then be used as input in the segmentation step.</p>"},{"location":"reconstruction/#available-tools","title":"Available tools","text":"<p>Several state-of-the-art super-resolution reconstruction algorithms have been wrapped and tested in fetpype. </p> Algorithm Repository Docker NiftyMIC<sup>1</sup> https://github.com/gift-surg/NiftyMIC https://hub.docker.com/r/renbem/niftymic SVRTK<sup>2</sup><sup>3</sup> https://github.com/SVRTK/SVRTK https://hub.docker.com/r/fetalsvrtk/svrtk NeSVoR<sup>4</sup> https://github.com/daviddmc/NeSVoR https://hub.docker.com/r/junshenxu/nesvor"},{"location":"reconstruction/#config-structure","title":"Config structure","text":"<p>Here's a typical structure found in the NeSVoR config.  <pre><code>pipeline: \"nesvor\"\ndocker: \n  cmd: \"docker run --gpus '\\\"device=0\\\"' &lt;mount&gt; junshenxu/nesvor:v0.5.0 \n    nesvor reconstruct \n    --input-stacks &lt;input_stacks&gt; \n    --stack-masks &lt;input_masks&gt; \n    --output-volume &lt;output_volume&gt; \n    --batch-size 4096 \n    --n-levels-bias 1\"\nsingularity:\n  cmd: \"singularity exec --bind &lt;singularity_mount&gt; --nv &lt;singularity_path&gt;/nesvor.sif \n    nesvor reconstruct \n    --input-stacks &lt;input_stacks&gt; \n    --stack-masks &lt;input_masks&gt; \n    --output-volume &lt;output_volume&gt; \n    --batch-size 4096 \n    --n-levels-bias 1\"\nargs:\n    path_to_output: \"nesvor.nii.gz\"\n</code></pre></p> <p>Note</p> <p>All the container runs use the command above and are passed through the function <code>run_recon_cmd</code></p>"},{"location":"reconstruction/#tags","title":"Tags","text":"<p>There are a limited set of tags that can be used for reconstruction: </p> Command Description Comments <code>&lt;mount&gt;</code> Where the different folders will be mounted on Docker Docker only <code>&lt;singularity_mount&gt;</code> Where the different folders will be mounted on Singularity Singularity only <code>&lt;input_stacks&gt;</code> The list of inputs stacks will be given as arguments Mutually exclusive with <code>&lt;input_dir&gt;</code> <code>&lt;input_dir&gt;</code> The folder that contains the input stacks Mutually exclusive with <code>&lt;input_stacks&gt;</code> <code>&lt;input_masks&gt;</code> The list of inputs masks will be given as arguments Mutually exclusive with <code>&lt;input_masks_dir&gt;</code> <code>&lt;input_masks_dir&gt;</code> The folder that contains the input masks Mutually exclusive with <code>&lt;input_masks&gt;</code> <code>&lt;output_volume&gt;</code> The output volume Mutually exclusive with <code>&lt;output_dir&gt;</code> <code>&lt;output_dir&gt;</code> The output directory Mutually exclusive with <code>&lt;output_volume&gt;</code> <code>&lt;input_tp&gt;</code> The through-plane resolution of input stacks Needed for SVRTK - Automatically calculated <code>&lt;output_res&gt;</code> The desired voxel resolution for the reconstructed volume This tag be set in the config file in the field <code>reconstruction/output_resolution</code>. <p>Note</p> <p>The configs contains an additional variable <code>path_to_output</code>. This is needed when only an <code>&lt;output_dir&gt;</code> is given to the method. This variable contains the path where the reconstructed volume will be located relative to <code>&lt;output_dir&gt;</code>.</p> <ol> <li> <p>Michael Ebner and others. An automated framework for localization, segmentation and super-resolution reconstruction of fetal brain mri. NeuroImage, 206:116324, 2020.\u00a0\u21a9</p> </li> <li> <p>Maria Kuklisova-Murgasova and others. Reconstruction of fetal brain MRI with intensity matching and complete outlier removal. Medical image analysis, 16(8):1550\u20131564, 2012.\u00a0\u21a9</p> </li> <li> <p>Alena U Uus and others. Automated 3D reconstruction of the fetal thorax in the standard atlas space from motion-corrupted MRI stacks for 21\u201336 weeks ga range. Medical image analysis, 80:102484, 2022.\u00a0\u21a9</p> </li> <li> <p>Junshen Xu and others. NeSVoR: implicit neural representation for slice-to-volume reconstruction in MRI. IEEE Transactions on Medical Imaging, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"run_parts/","title":"Running parts of the pipeline","text":"<p>Fetpype comes with three main scripts, located in the <code>workflows</code> folder, than can be called from command line after installing fetpype.</p> Command Python script Description Input <code>fetpype_run</code> <code>pipeline_fet.py</code> Run the entire fetpype pipeline: pre-processing, reconstruction and segmentation T2-weighted stacks <code>fetpype_run_rec</code> <code>pipeline_rec.py</code> Run the pre-processing and reconstruction pipeline T2-weighted stacks <code>fetpype_run_seg</code> <code>pipeline_rec.py</code> Run the segmentation pipeline Super-resolution reconstructed volume <p>Note</p> <p>We recommend you to start by running the entire pipeline with default parameters on a few subjects, to see how the outputs will be structured. This will be useful in particular when running only the segmentation pipeline (<code>fetpype_run_seg</code>), as your data need to be properly formatted for the script to work as expected. More information on output formatting is available here.</p>"},{"location":"segmentation/","title":"Segmentation","text":"<p>In the segmentation step, a super-resolution reconstructed volume is provided to a segmentation algorithm that outputs a parcellation of the brain into several regions. The segmented volume can then be used as input to surface extraction.</p>"},{"location":"segmentation/#available-tools","title":"Available tools","text":"<p>Several state-of-the-art segmentation algorithms have been wrapped and tested in fetpype.</p> Algorithm Repository Docker BOUNTI<sup>1</sup> https://github.com/SVRTK/auto-proc-svrtk https://hub.docker.com/r/fetalsvrtk/segmentation dHCP<sup>2</sup> https://github.com/fetpype/dhcp-structural-pipeline https://hub.docker.com/r/gerardmartijuan/dhcp-pipeline-multifact <p>\u26a0\ufe0f Disclaimer: The dHCP pipeline is only available in the dev branch as of now.</p> <p>The version of the algorithm used in fetpype is the one available on the Docker Hub, which have some changes to the original code. The repository is available at https://github.com/fetpype/dhcp-structural-pipeline.</p>"},{"location":"segmentation/#config-structure","title":"Config structure","text":"<p>Here's a typical structure found in the BOUNTI config.  <pre><code>pipeline: \"bounti\"\ndocker: \n  cmd: \"docker run --rm &lt;mount&gt;\n    fetalsvrtk/segmentation:general_auto_amd \n    bash /home/auto-proc-svrtk/scripts/auto-brain-bounti-segmentation-fetal.sh \n    &lt;input_dir&gt; &lt;output_dir&gt;\"\nsingularity:\n  cmd: \"singularity exec -u --bind &lt;singularity_mount&gt; --bind &lt;singularity_home&gt;:/home/tmp_proc --nv\n    &lt;singularity_path&gt;/bounti.sif\n    bash /home/auto-proc-svrtk/auto-brain-bounti-segmentation-fetal.sh \n    &lt;input_dir&gt; &lt;output_dir&gt;\"\npath_to_output: \"&lt;basename&gt;-mask-brain_bounti-19.nii.gz\"\n</code></pre></p> <p>Note</p> <p>All the container runs use the command above and are passed through the function <code>run_seg_cmd</code></p>"},{"location":"segmentation/#tags","title":"Tags","text":"<p>There are a limited set of tags that can be used for reconstruction: </p> Command Description Comments <code>&lt;mount&gt;</code> Where the different folders will be mounted on Docker Docker only <code>&lt;singularity_mount&gt;</code> Where the different folders will be mounted on Singularity Singularity only <code>&lt;singularity_path&gt;</code> The base path of the Singularity image Singularity only <code>&lt;singularity_home&gt;</code> A directory used for temporary files Singularity only <code>&lt;input_stacks&gt;</code> The list of inputs stacks will be given as arguments Mutually exclusive with <code>&lt;input_dir&gt;</code> <code>&lt;input_dir&gt;</code> The folder that contains the input stacks Mutually exclusive with <code>&lt;output_dir&gt;</code> <code>&lt;output_dir&gt;</code> The output directory Mutually exclusive with"},{"location":"segmentation/#dhcp-processing-pipeline","title":"dHCP processing pipeline","text":"<p>The dHCP pipeline presents some particularities compared to the BOUNTI implementation.</p>"},{"location":"segmentation/#gestational-age-requirement","title":"Gestational Age Requirement","text":"<p>The dHCP pipeline requires gestational age information, which can be provided through the <code>participants.tsv</code> file in the root of the BIDS dataset:</p> <pre><code>participant_id    gestational_age\nsub-01           28.5\nsub-02           32.1\nsub-03           25.8\n</code></pre>"},{"location":"segmentation/#processing-stages","title":"Processing Stages","text":"<p>You can choose to run only the segmentation, the surface reconstruction, or both. The default is to run both. The following options are available, and you should add them to the \"cmd\" field of the configuration file:</p> <p><code>-seg</code> (Segmentation only):</p> <p><code>-surf</code> (Surface reconstruction):</p> <p><code>-all</code> (Complete Processing):</p>"},{"location":"segmentation/#known-dhcp-issues","title":"Known dHCP issues","text":"<p>The pipeline has been shown to fail in specific systems. We are still investigating the cause of this issue, but if the pipeline fails with the following error in logs/-tissue-em-err.log: <pre><code>10%...Error: draw-em command returned non-zero exit status -8\n</code></pre> <p>Try to run the pipeline in a different system or, if you are in an HPC, in a different node. If the problem persists, please reach out to the original authors of the tool on this GitHub repository.</p> <ol> <li> <p>Alena U Uus and others. BOUNTI: brain volumetry and automated parcellation for 3d fetal mri. bioRxiv, 2023.\u00a0\u21a9</p> </li> <li> <p>Antonios Makropoulos, Emma C Robinson, Andreas Schuh, Robert Wright, Sean Fitzgibbon, Jelena Bozek, Serena J Counsell, Johannes Steinweg, Katy Vecchiato, Jonathan Passerat-Palmbach, and others. The developing human connectome project: a minimal processing pipeline for neonatal cortical surface reconstruction. Neuroimage, 173:88\u2013112, 2018.\u00a0\u21a9</p> </li> </ol>"},{"location":"surface/","title":"Surface Extraction","text":"<p>In the surface extraction step, a segmented volume is processed to build a 3D surface of the white matter surface of the brain. This is the last step of fetpype.</p>"},{"location":"surface/#segmentation","title":"Segmentation","text":""},{"location":"surface/#available-tools","title":"Available tools","text":"<p>Several state-of-the-art segmentation algorithms have been wrapped and tested in <code>fetpype</code>.</p> Algorithm Repository Docker Fetpype surface extraction (<code>surf_pype</code>) https://github.com/fetpype/surface_processing https://hub.docker.com/r/fetpype/surf_proc dHCP<sup>1</sup> https://github.com/fetpype/dhcp-structural-pipeline https://hub.docker.com/r/gerardmartijuan/dhcp-pipeline-multifact <p>\u26a0\ufe0f Disclaimer: The dHCP pipeline is only available in the dev branch as of now. Surface extraction using the dHCP structural pipeline<sup>1</sup> can be obtained with the flags <code>-surf</code> and <code>-all</code>.</p>"},{"location":"surface/#config-structure","title":"Config structure","text":"<p>Here's a typical structure found in the BOUNTI config.  <pre><code>pipeline: \"surf_pype\"\ndocker: \n  cmd: \"docker run --rm &lt;mount&gt;\n    fetpype/surf_proc:v0.0.2\n    generate_mesh -l &lt;labelling_scheme&gt; \n    -s &lt;input_seg&gt; \n    -m &lt;output_surf&gt;\"\nsingularity:\n  cmd: \"singularity exec --bind &lt;singularity_mount&gt; --home &lt;singularity_home&gt; --nv\n    &lt;singularity_path&gt;/macatools/surf_proc.sif\n    fetpype/surf_proc:v0.0.2\n    generate_mesh -l &lt;labelling_scheme&gt; \n    -s &lt;input_seg&gt; \n    -m &lt;output_surf&gt;\"\n\nsurface_lh:\n    use_scheme: \"bounti\"\n    out_file: \"hemi-L_white.surf.gii\"\n    labelling_scheme:\n        bounti: [5, 7, 14, 16]\n\nsurface_rh:\n    use_scheme: \"bounti\"\n    out_file: \"hemi-R_white.surf.gii\"\n    labelling_scheme:\n        bounti: [6, 8, 15, 17]\n</code></pre></p> <p>Note</p> <p>The surface is computed on each hemisphere separately (<code>surface_lh</code> and <code>surface_rh</code>). The surface extraction groups together labels in the white matter. For the left hemisphere and BOUNTI labels, it groups the left white matter (5), the left lateral ventricle (7), the left basal ganglia (14) as well as the left thalamus (16). Information on the BOUNTI labelling scheme are available here.</p> <p>Note</p> <p>All the container runs use the command above and are passed through the function <code>run_surf_cmd</code></p>"},{"location":"surface/#tags","title":"Tags","text":"<p>There are a limited set of tags that can be used for reconstruction: </p> Command Description Comments <code>&lt;mount&gt;</code> Where the different folders will be mounted on Docker Docker only <code>&lt;singularity_mount&gt;</code> Where the different folders will be mounted on Singularity Singularity only <code>&lt;singularity_path&gt;</code> The base path of the Singularity image Singularity only <code>&lt;singularity_home&gt;</code> A directory used for temporary files Singularity only <code>&lt;input_seg&gt;</code> The input segmentation to be used for surface extraction <code>&lt;labelling_scheme&gt;</code> List of labels in the  to concatenate in order to get the white matter mask of the given hemisphere <code>&lt;output_surf&gt;</code> The output extracted surface"},{"location":"surface/#available-tools_1","title":"Available tools","text":"<p>Surface extraction in Fetpype is currenly available using the dHCP structural pipeline<sup>1</sup>, with the flags <code>-surf</code> and <code>-all</code>.</p> <p>Note: Surface extraction requires prior reconstruction and gestational age information. Refer to the dHCP configuration and requirements for more details.</p> <ol> <li> <p>Antonios Makropoulos, Emma C Robinson, Andreas Schuh, Robert Wright, Sean Fitzgibbon, Jelena Bozek, Serena J Counsell, Johannes Steinweg, Katy Vecchiato, Jonathan Passerat-Palmbach, and others. The developing human connectome project: a minimal processing pipeline for neonatal cortical surface reconstruction. Neuroimage, 173:88\u2013112, 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"}]}